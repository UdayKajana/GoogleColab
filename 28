python3 pipeline.py \
--project vz-it-np-gudv-dev-vzntdo-0 \
--runner DataflowRunner \
--region us-east4 \
--parameters query="SELECT * FROM vz-it-np-gudv-dev-vzntdo-0.vzn_nsdl_common_core_tbls.echo_ticket_opened_proc_v0 LIMIT 10" \
--staging_location gs://vznet-test/wireline_churn_bq_spanner/stg \
--temp_location gs://vznet-test/wireline_churn_test/tmp/ \
--template_location gs://vznet-test/wireline_churn_test/code/wireline_churn_pubsub_csv
Traceback (most recent call last):
  File "/home/udayka/workspace/pipeline.py", line 39, in <module>
    run()
  File "/home/udayka/workspace/pipeline.py", line 27, in run
    with beam.Pipeline(options=pipeline_options) as pipeline:
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/pipeline.py", line 210, in __init__
    errors = PipelineOptionsValidator(self._options, runner).validate()
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/options/pipeline_options_validator.py", line 149, in validate
    errors.extend(self.options.view_as(cls).validate(self))
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/options/pipeline_options.py", line 1032, in validate
    errors.extend(validator.validate_cloud_options(self))
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/options/pipeline_options_validator.py", line 210, in validate_cloud_options
    if self.is_full_string_match(self.PROJECT_NUMBER_PATTERN, project):
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/options/pipeline_options_validator.py", line 171, in is_full_string_match
    return re.search(pattern, string) is not None
  File "/usr/lib64/python3.9/re.py", line 201, in search
    return _compile(pattern, flags).search(string)
TypeError: expected string or bytes-like object
import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.options.pipeline_options import StandardOptions
from apache_beam.options.pipeline_options import GoogleCloudOptions
from apache_beam.io.gcp.bigquery import ReadFromBigQuery

def run():
    # Define custom options
    class CustomOptions(PipelineOptions):
        @classmethod
        def _add_argparse_args(cls, parser):
            parser.add_value_provider_argument('--query', type=str, help='BigQuery SQL query')
            parser.add_value_provider_argument('--project', type=str, help='Google Cloud project ID')

    # Parse command line arguments
    pipeline_options = CustomOptions()

    # Ensure project is set
    google_cloud_options = pipeline_options.view_as(GoogleCloudOptions)
    if not google_cloud_options.project:
        raise ValueError("Project must be specified.")

    # Set runner (DataflowRunner for cloud execution)
    pipeline_options.view_as(StandardOptions).runner = 'DataflowRunner'

    # Create pipeline
    with beam.Pipeline(options=pipeline_options) as pipeline:
        # Read from BigQuery using the provided query
        records = pipeline | 'ReadFromBigQuery' >> ReadFromBigQuery(
            query=pipeline_options.query,
            use_standard_sql=True,
            gcs_location=pipeline_options.temp_location
        )

        # Add your transformations here
        # Example: records | 'SomeTransform' >> beam.Map(some_function)

if __name__ == '__main__':
    run()
