import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.options.pipeline_options import StandardOptions
from apache_beam.options.pipeline_options import GoogleCloudOptions
from apache_beam.io.gcp.bigquery import ReadFromBigQuery

def run():
    # Define pipeline options
    pipeline_options = PipelineOptions()

    # Validate Google Cloud options
    google_cloud_options = pipeline_options.view_as(GoogleCloudOptions)
    if not google_cloud_options.project:
        raise ValueError("Project must be specified.")
    if not google_cloud_options.temp_location:
        raise ValueError("Temporary GCS location must be specified.")
    if not google_cloud_options.staging_location:
        raise ValueError("Staging GCS location must be specified.")

    # Set runner (DataflowRunner for cloud execution)
    pipeline_options.view_as(StandardOptions).runner = 'DataflowRunner'

    # Debug logs for verification
    print(f"Project: {google_cloud_options.project}")
    print(f"Region: {google_cloud_options.region}")
    print(f"Temp Location: {google_cloud_options.temp_location}")
    print(f"Staging Location: {google_cloud_options.staging_location}")

    # Create pipeline
    with beam.Pipeline(options=pipeline_options) as pipeline:
        # Read query from pipeline options
        query = pipeline_options.view_as(GoogleCloudOptions).query
        if not query:
            raise ValueError("Query must be specified.")

        # Read from BigQuery using the provided query
        records = pipeline | 'ReadFromBigQuery' >> ReadFromBigQuery(
            query=query,
            use_standard_sql=True,
            gcs_location=google_cloud_options.temp_location
        )

        # Add your transformations here (example placeholder)
        transformed_records = records | 'ExampleTransform' >> beam.Map(lambda record: record)

        # Write transformed records or add further processing if needed

if __name__ == '__main__':
    run()

{
  "project": "vz-it-np-gudv-dev-vzntdo-0",
  "region": "us-east4",
  "staging_location": "gs://vznet-test/wireline_churn_bq_spanner/stg",
  "temp_location": "gs://vznet-test/wireline_churn_test/tmp/",
  "query": "SELECT * FROM `vz-it-np-gudv-dev-vzntdo-0.vzn_nsdl_common_core_tbls.echo_ticket_opened_proc_v0` LIMIT 10"
}


gcloud dataflow flex-template build gs://vznet-test/wireline_churn_test/code/wireline_churn_pubsub_csv \
    --image "gcr.io/your-project-id/your-image-name:tag" \
    --sdk-language "PYTHON" \
    --metadata-file "metadata.json" \
    --python-file "pipeline.py"

gcloud dataflow flex-template run "your-job-name" \
    --template-file-gcs-location "gs://vznet-test/wireline_churn_test/code/wireline_churn_pubsub_csv" \
    --parameters-file parameters.json
