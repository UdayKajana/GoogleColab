import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.options.pipeline_options import StandardOptions
from apache_beam.options.pipeline_options import GoogleCloudOptions
from apache_beam.io.gcp.bigquery import ReadFromBigQuery
import argparse
import logging

class CustomOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
        parser.add_argument(
            '--query',
            required=True,
            help='BigQuery SQL query to execute'
        )

def run():
    """Main entry point."""
    # Set up logging
    logging.getLogger().setLevel(logging.INFO)
    
    # Initialize pipeline options
    pipeline_options = PipelineOptions()
    
    # Get custom options
    custom_options = pipeline_options.view_as(CustomOptions)
    
    # Get Google Cloud options and validate
    google_cloud_options = pipeline_options.view_as(GoogleCloudOptions)
    # if not google_cloud_options.project:
    google_cloud_options.project = "vz-it-np-gudv-dev-vzntdo-0"
    
    logging.info(f"Starting pipeline with project: {google_cloud_options.project}")
    logging.info(f"Query: {custom_options.query}")
    
    # Create pipeline
    pipeline = beam.Pipeline(options=pipeline_options)
    
    # Read from BigQuery
    records = pipeline | 'ReadFromBigQuery' >> ReadFromBigQuery(
        query=custom_options.query,
        use_standard_sql=True,
        gcs_location=google_cloud_options.temp_location
    )
    
    # Add transformations
    transformed_records = records | 'ExampleTransform' >> beam.Map(lambda record: record)
    
    # Run the pipeline
    result = pipeline.run()
    result.wait_until_finish()

if __name__ == '__main__':
    run()



# Build the Docker image (unchanged)
gcloud builds submit --tag us-east4-docker.pkg.dev/vz-it-np-gudv-dev-vzntdo-0/vznet/pipeline_env:V1 .

# Build the Flex Template (unchanged)
gcloud dataflow flex-template build gs://vznet-test/wireline_churn_test/src/template.json \
    --image us-east4-docker.pkg.dev/vz-it-np-gudv-dev-vzntdo-0/vznet/pipeline_env:V1 \
    --sdk-language "PYTHON" \
    --metadata-file metadata.json

# Run the Flex Template (corrected)
gcloud dataflow flex-template run bigquery-pipeline \
    --template-file-gcs-location gs://vznet-test/wireline_churn_test/src/template.json \
    --project vz-it-np-gudv-dev-vzntdo-0 \
    --region us-east4 \
    --parameters query="SELECT * FROM vz-it-np-gudv-dev-vzntdo-0.vzn_nsdl_common_core_tbls.echo_ticket_opened_proc_v0 LIMIT 10" \
    --num-workers 2 \
    --max-workers 2 \
    --worker-machine-type n2-standard-2 \
    --disable-public-ips \
    --network shared-np-east \
    --subnetwork https://www.googleapis.com/compute/v1/projects/vz-it-np-exhv-sharedvpc-228116/regions/us-east4/subnetworks/shared-np-east-green-subnet-2 \
    --service-account-email sa-dev-gudv-app-vzntdo-0@vz-it-np-gudv-dev-vzntdo-0.iam.gserviceaccount.com \
    --temp-location gs://vznet-test/wireline_churn_test/tmp/ \
    --dataflow-kms-key projects/vz-it-np-d0sv-vsadkms-0/locations/us-east4/keyRings/vz-it-np-kr-aid/cryptoKeys/vz-it-np-kms-gudv
