import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.options.pipeline_options import StandardOptions
import argparse
import logging

logging.getLogger().setLevel(logging.ERROR)

class TemplateOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
        # Only add custom arguments that aren't already defined in PipelineOptions
        parser.add_argument('--query', dest='query', required=True, help='BigQuery SQL query')
        parser.add_argument('--gcs_folder_path', dest='gcs_folder_path', required=True, help='Output GCS folder path')

def run(argv=None):
    parser = argparse.ArgumentParser()
    known_args, pipeline_args = parser.parse_known_args(argv)
    
    pipeline_options = TemplateOptions(pipeline_args)
    pipeline_options.view_as(StandardOptions).runner = 'DataflowRunner'
    
    with beam.Pipeline(options=pipeline_options) as p:
        # Reading from BigQuery
        data = p | 'ReadFromBigQuery' >> beam.io.ReadFromBigQuery(
            query=pipeline_options.query,
            use_standard_sql=True
        )
        
        # Converting to CSV format
        csv_data = data | 'ConvertToCSV' >> beam.Map(
            lambda row: ','.join(str(x) for x in row.values())
        )
        
        # Writing to GCS
        csv_data | 'WriteToGCS' >> beam.io.WriteToText(
            f'{pipeline_options.gcs_folder_path}/output.csv',
            file_name_suffix='.csv',
            header="ont_activation_date,data_circuit_id,circuit_id,video_circuit_id,service_type,address_id,vision_account_id,vision_customer_id,address_type,line_of_business"
        )

if __name__ == '__main__':
    run()


FROM gcr.io/dataflow-templates-base/python39-template-launcher-base
WORKDIR /template
RUN apt-get update && apt-get install -y \
    build-essential \
    libffi-dev \
    libssl-dev \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*
COPY requirements.txt /template/requirements.txt
COPY pipeline.py /template/pipeline.py
RUN cat /template/pipeline.py
ENV DATAFLOW_SERVICE_ACCOUNT="sa-dev-gudv-app-vzntdo-0@vz-it-np-gudv-dev-vzntdo-0.iam.gserviceaccount.com"
ENV FLEX_TEMPLATE_PYTHON_PY_FILE="/template/pipeline.py"
ENV FLEX_TEMPLATE_PYTHON_REQUIREMENTS_FILE="/template/requirements.txt"
ENV FLEX_TEMPLATE_PYTHON_SETUP_FILE=""
ENV FLEX_TEMPLATE_PYTHON_OPTIONS="--runner=DataflowRunner"



{
    "name": "BigQuery to GCS Pipeline",
    "description": "A pipeline that reads from BigQuery and writes to GCS",
    "parameters": [
        {
            "name": "query",
            "label": "BigQuery SQL query",
            "helpText": "SQL query to execute",
            "paramType": "TEXT",
            "isOptional": false
        },
        {
            "name": "gcs_folder_path",
            "label": "Output GCS folder path",
            "helpText": "GCS location to write output files",
            "paramType": "TEXT",
            "isOptional": false
        },
        {
            "name": "network",
            "label": "VPC Network",
            "helpText": "The VPC network to use",
            "paramType": "TEXT",
            "isOptional": false
        },
        {
            "name": "subnetwork",
            "label": "VPC Subnetwork",
            "helpText": "The VPC subnetwork to use",
            "paramType": "TEXT",
            "isOptional": false
        },
        {
            "name": "service_account_email",
            "label": "Service Account Email",
            "helpText": "Service account email to run the job",
            "paramType": "TEXT",
            "isOptional": false
        }
    ]
}
