import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.options.pipeline_options import StandardOptions
import argparse
import logging

logging.getLogger().setLevel(logging.ERROR)

class TemplateOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
        # Only add custom arguments that aren't already defined in PipelineOptions
        parser.add_argument('--query', dest='query', required=True, help='BigQuery SQL query')
        parser.add_argument('--gcs_folder_path', dest='gcs_folder_path', required=True, help='Output GCS folder path')

def run(argv=None):
    parser = argparse.ArgumentParser()
    known_args, pipeline_args = parser.parse_known_args(argv)
    
    pipeline_options = TemplateOptions(pipeline_args)
    pipeline_options.view_as(StandardOptions).runner = 'DataflowRunner'
    
    with beam.Pipeline(options=pipeline_options) as p:
        # Reading from BigQuery
        data = p | 'ReadFromBigQuery' >> beam.io.ReadFromBigQuery(
            query=pipeline_options.query,
            use_standard_sql=True
        )
        
        # Converting to CSV format
        csv_data = data | 'ConvertToCSV' >> beam.Map(
            lambda row: ','.join(str(x) for x in row.values())
        )
        
        # Writing to GCS
        csv_data | 'WriteToGCS' >> beam.io.WriteToText(
            f'{pipeline_options.gcs_folder_path}/output.csv',
            file_name_suffix='.csv',
            header="ont_activation_date,data_circuit_id,circuit_id,video_circuit_id,service_type,address_id,vision_account_id,vision_customer_id,address_type,line_of_business"
        )

if __name__ == '__main__':
    run()
