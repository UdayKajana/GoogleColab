import argparse
import json
import logging
import time
from io import BytesIO
import apache_beam as beam
from apache_beam import pvalue as pvl
from apache_beam.transforms.window import FixedWindows
from apache_beam.io import WriteToText
from apache_beam.io.gcp.pubsub import ReadFromPubSub
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.io.gcp.gcsio import GcsIO

# Set logging level
logging.getLogger().setLevel(logging.INFO)

class TemplateOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
        parser.add_value_provider_argument('--params', type=str)

# Parsing command-line arguments
parser = argparse.ArgumentParser()
parser.add_argument('--project', dest='project', required=True, help='GCP Project ID')
parser.add_argument('--runner', dest='runner', required=False, default="DataflowRunner", help='Pipeline runner')
parser.add_argument('--region', dest='region', required=False, default="us-east4", help='GCP project region')
parser.add_argument('--staging_location', dest='staging_location', required=True, help='Staging GCS bucket path')
parser.add_argument('--temp_location', dest='temp_location', required=True, help='Temp GCS bucket path')
parser.add_argument('--template_location', dest='template_location', required=True, help='GCS bucket path to save template')
parser.add_argument('--input_sub', dest='input_sub', required=True, help='Input Subscription')
parser.add_argument('--sdk_container_image', dest='sdk_container_image',
                    default='us-east4-docker.pkg.dev/vz-it-np-gudv-dev-vzntdo-0/vznet/beamredis:1.0.0',
                    required=False, help='SDK container image location')
parser.add_argument('--gcs_folder_path', dest='gcs_folder_path', required=True, help='GCS folder path to write data')
known_args, beam_args = parser.parse_known_args()

# Decode Avro records
class DecodeAvroRecords(beam.DoFn):
    def process(self, element):
        def reformat_input_msg_schema(msg):
            return {
                'timestamp': msg.get('timestamp'),
                'host': msg.get('host'),
                'src': msg.get('src'),
                'ingressTimestamp': msg.get('_event_ingress_ts'),
                'origins': [msg.get('_event_origin')],
                'tags': msg.get('_event_tags'),
                'route': 3,
                'fetchTimestamp': int(time.time() * 1000),
                'rawdata': msg.get('rawdata'),
            }
        
        raw_schema = """{
            "namespace": "com.vz.vznet",
            "type": "record",
            "name": "VznetDefault",
            "doc": "Default schema for events in transit",
            "fields": [
                {"name": "timestamp", "type": "long"},
                {"name": "host", "type": "string"},
                {"name": "src", "type": "string"},
                {"name": "_event_ingress_ts", "type": "long"},
                {"name": "_event_origin", "type": "string"},
                {"name": "_event_tags", "type": {"type": "array", "items": "string"}},
                {"name": "rawdata", "type": "bytes"}
            ]
        }"""
        try:
            schema = avro.schema.parse(raw_schema)
            avro_reader = avro.io.DatumReader(schema)
            avro_decoder = avro.io.BinaryDecoder(BytesIO(element))
            message = avro_reader.read(avro_decoder)
            message['rawdata'] = message['rawdata'].decode("utf-8")
            reformatted_message = reformat_input_msg_schema(message)
            yield pvl.TaggedOutput("valid_recs", reformatted_message)
        except Exception as e:
            logging.error(f"Error decoding Avro message: {e}")
            yield pvl.TaggedOutput("invalid_recs", element)

# Add timestamp to records
class AddTimestampDoFn(beam.DoFn):
    def process(self, element):
        timestamp = element.get('timestamp', int(time.time()))
        yield beam.window.TimestampedValue(element, timestamp)

# Convert dictionary to CSV-like string
def dict_to_str(row):
    return ",".join([str(value) for value in row.values()])

# Define headers for the output file
headers = {
    'vrepair_ticket_opened_norm_v0': "telephone_number,trouble_report_num,address_id,chronic_flag,chronic_total,line_id_trimmed,port_associated_service,date_opened,data_circuit_id,video_circuit_id"
}

# Define pipeline options
options = {
    'project': known_args.project,
    'runner': known_args.runner,
    'region': known_args.region,
    'staging_location': known_args.staging_location,
    'temp_location': known_args.temp_location,
    'template_location': known_args.template_location,
    'save_main_session': True,
    'streaming': True,
    'sdk_container_image': known_args.sdk_container_image
}

pipeline_options = PipelineOptions.from_dictionary(options)
p = beam.Pipeline(options=pipeline_options)
pubsub_data = (p 
    | 'ReadFromPubSub' >> ReadFromPubSub(
        subscription=f"projects/{known_args.project}/subscriptions/{known_args.input_sub}",
        with_attributes=True  # Ensure message attributes are preserved
    )
    | 'Decode Msg' >> beam.ParDo(DecodeAvroRecords()).with_outputs()
)

# Convert to KV pairs for proper windowing
def create_key_value(element):
    # Create a key based on relevant fields
    key = f"{element.get('host')}_{element.get('src')}"
    return (key, element)
import argparse
import json
import logging
import time
from io import BytesIO
import apache_beam as beam
from apache_beam import pvalue as pvl
from apache_beam.transforms.window import FixedWindows
from apache_beam.io import WriteToText
from apache_beam.io.gcp.pubsub import ReadFromPubSub
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.io.gcp.gcsio import GcsIO
from apache_beam.transforms.trigger import AfterProcessingTime, AccumulationMode, AfterWatermark
windowed_data = (pubsub_data.valid_recs
    | 'CreateKeyValue' >> beam.Map(create_key_value)
    | 'Add Timestamp' >> beam.ParDo(AddTimestampDoFn())
    | 'Window' >> beam.WindowInto(
        FixedWindows(60),
        trigger=AfterWatermark(
            early=AfterProcessingTime(10),
            late=AfterProcessingTime(30)
        ),
        allowed_lateness=60,  # Allow 1 minute of late data
        accumulation_mode=AccumulationMode.DISCARDING
    )
    | 'GroupByKey' >> beam.GroupByKey()
    | 'Flatten' >> beam.FlatMap(lambda x: x[1])  # Flatten grouped values
)

# Convert to string and write output
output_data = (windowed_data
    | 'DictToStr' >> beam.Map(dict_to_str)
    | 'WriteToFile' >> WriteToText(
        known_args.gcs_folder_path,
        file_name_suffix=".csv",
        header=headers.get("vrepair_ticket_opened_norm_v0"),
        num_shards=10  # Adjust based on your data volume
    )
)

# Error logging
print_invalid = (pubsub_data.invalid_recs 
    | 'InvalidLogs' >> beam.Map(lambda r: logging.error(f"Invalid record: {r}")))

p.run().wait_until_finish()
# # Build the pipeline
# pubsub_data = (p 
#     | 'ReadFromPubSub' >> ReadFromPubSub(subscription=f"projects/{known_args.project}/subscriptions/{known_args.input_sub}")
#     | 'Decode Msg' >> beam.ParDo(DecodeAvroRecords()).with_outputs()
# )

# str_row = pubsub_data.valid_recs | 'DictToStr' >> beam.Map(dict_to_str)

# from apache_beam.transforms.trigger import AfterProcessingTime, AccumulationMode, AfterWatermark

# # Apply windowing
# win_str_row = (str_row
#     | 'Add Timestamp' >> beam.ParDo(AddTimestampDoFn())
#     | 'Windowing' >> beam.WindowInto(
#         FixedWindows(60),
#         trigger=beam.transforms.trigger.AfterWatermark(
#             early=beam.transforms.trigger.AfterProcessingTime(10),
#             late=beam.transforms.trigger.AfterProcessingTime(30)
#         ),
#         accumulation_mode=AccumulationMode.DISCARDING
#     )
# )

# op = (win_str_row 
#     | 'Write-To-File' >> WriteToText(
#         known_args.gcs_folder_path,
#         file_name_suffix=".csv",
#         header=headers.get("vrepair_ticket_opened_norm_v0")
#     )
# )

# print_row = (str_row | 'Logs' >> beam.Map(lambda r: logging.info(r)))
# print_invalid = (pubsub_data.invalid_recs | 'InvalidLogs' >> beam.Map(lambda r: logging.info(r)))

# # Run the pipeline
# p.run().wait_until_finish()

# logging.info(f"Created template at {known_args.template_location}.")


INFO:apache_beam.typehints.native_type_compatibility:Converting string literal type hint to Any: "PubsubMessage"
Traceback (most recent call last):
  File "/home/udayka/model_scores_prospect_hist/vrepair_ticket_opened_norm_v1.py", line 153, in <module>
    output_data = (windowed_data
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/pvalue.py", line 138, in __or__
    return self.pipeline.apply(ptransform, self)
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/pipeline.py", line 681, in apply
    return self.apply(
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/pipeline.py", line 692, in apply
    return self.apply(transform, pvalueish)
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/pipeline.py", line 754, in apply
    pvalueish_result = self.runner.apply(transform, pvalueish, self._options)
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/runners/runner.py", line 191, in apply
    return self.apply_PTransform(transform, input, options)
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/runners/runner.py", line 195, in apply_PTransform
    return transform.expand(input)
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/io/textio.py", line 895, in expand
    return pcoll | Write(self._sink)
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/pvalue.py", line 138, in __or__
    return self.pipeline.apply(ptransform, self)
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/pipeline.py", line 754, in apply
    pvalueish_result = self.runner.apply(transform, pvalueish, self._options)
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/runners/runner.py", line 191, in apply
    return self.apply_PTransform(transform, input, options)
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/runners/runner.py", line 195, in apply_PTransform
    return transform.expand(input)
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/io/iobase.py", line 1070, in expand
    return pcoll | WriteImpl(self.sink)
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/pvalue.py", line 138, in __or__
    return self.pipeline.apply(ptransform, self)
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/pipeline.py", line 754, in apply
    pvalueish_result = self.runner.apply(transform, pvalueish, self._options)
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/runners/runner.py", line 191, in apply
    return self.apply_PTransform(transform, input, options)
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/runners/runner.py", line 195, in apply_PTransform
    return transform.expand(input)
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/io/iobase.py", line 1140, in expand
    keyed_pcoll
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/pvalue.py", line 138, in __or__
    return self.pipeline.apply(ptransform, self)
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/pipeline.py", line 754, in apply
    pvalueish_result = self.runner.apply(transform, pvalueish, self._options)
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/runners/runner.py", line 191, in apply
    return self.apply_PTransform(transform, input, options)
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/runners/runner.py", line 195, in apply_PTransform
    return transform.expand(input)
  File "/apps/opt/application/vznet/pyvenv39/lib64/python3.9/site-packages/apache_beam/transforms/core.py", line 3254, in expand
    raise ValueError(
ValueError: GroupByKey cannot be applied to an unbounded PCollection with global windowing and a default trigger
