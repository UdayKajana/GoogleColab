import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.options.pipeline_options import StandardOptions
import argparse
import logging

logging.getLogger().setLevel(logging.ERROR)

class PrepareData(beam.DoFn):
    def __init__(self, process_name):
        self.process_name = process_name
    def dim_inventory_customer_profiles_norm_v0(self,row):
        return ','.join([str(row.get(col, '')) for col in ["ont_activation_date", "data_circuit_id", "circuit_id", "video_circuit_id", "service_type", "address_id", "vision_account_id", "vision_customer_id", "address_type", "line_of_business"]])
    
    def vrepair_ticket_opened_norm_v0(self,element):
        import avro.io as avro_io
        import avro.schema
        from io import BytesIO
        import time
        import datetime
        message: dict = {}
        raw_schema = """{"namespace": "com.vz.vznet",
                                           "type": "record",
                                           "name": "VznetDefault",
                                           "doc": "Default schema for events in transit",
                                           "fields": [
                                           {"name": "timestamp", "type": "long"},
                                           {"name": "host", "type": "string"},
                                           {"name": "src",  "type": "string" },
                                           {"name": "_event_ingress_ts", "type": "long"},
                                           {"name": "_event_origin", "type": "string"},
                                           {"name": "_event_tags", "type": {"type": "array", "items": "string"}},
                                           {"name": "_event_route", "type": "string"},
                                           {"name": "_event_metrics", "type": ["null", "bytes"], "default": null},
                                           {"name": "rawdata", "type": "bytes"}
                                           ]
                                           }"""
        try:
            if isinstance(raw_schema, dict):
                raw_schema = json.dumps(raw_schema)
            schema = avro.schema.parse(raw_schema)
            avro_reader: avro_io.DatumReader = avro_io.DatumReader(schema)
            avro_message: avro_io.BinaryDecoder = avro_io.BinaryDecoder(BytesIO(element))
            message = avro_reader.read(avro_message)
            if schema.name == 'VznetDefault':
                message['rawdata'] = message['rawdata'].decode("utf-8")
                if message['_event_metrics'] is None:
                    message['_event_metrics'] = message['_event_metrics']
                else:
                    message['_event_metrics'] = message['_event_metrics'].decode("utf-8")
        except ValueError as e:
            logging.log(logging.INFO, f"Error deserializing the records :: {e}")
        reformatted = self.reformat_input_msg_schema(message)
        return reformatted
        # return ','.join([str(row.get(col, '')) for col in ["telephone_number","trouble_report_num","address_id","chronic_flag","chronic_total","line_id_trimmed","port_associated_service","date opened ","data_circuit_id","video_circuit_id"]])
    
    def dom_cdi_keys_xref_dom_fact_sls_to_prov_walk(self,row):
        return ','.join([str(row.get(col, '')) for col in ["acct_sk","mon","cust_sk","svc_party_sk","acct_estbd_dt","acct_term_dt"]])
    def model_scores_prospect_hist(self,row):
        return ','.join([str(row.get(col, '')) for col in ["addr_id","state_cd","model_id","model_version","round_cycle","model_score","model_decile","model_centile","model_segment","model_subsegment","scoring_driver_1","scoring_driver_2","scoring_driver_3","create_date"]])
    def wln_model_scores(row):
        return ','.join([str(row.get(col, '')) for col in ["acct_sk","acct_type_cd","create_date","epsilon_cust_id","insert_ts","model_centile","model_decile","model_id","model_score","model_subsegment","model_version","round_cycle","scoring_driver_1","scoring_driver_2","scoring_driver_3"]])
    
    def reformat_input_msg_schema(msg): 
        fmt_msg = {}
        fmt_msg['timestamp'] = msg['timestamp']
        fmt_msg['host'] = msg['host']
        fmt_msg['src'] = msg['src']
        fmt_msg['ingressTimestamp'] = msg['_event_ingress_ts']
        fmt_msg['origins'] = [msg['_event_origin']]
        tags_len = len(msg['_event_tags'])
        if tags_len > 0:
            if msg['_event_tags'][0] != "" and msg['_event_tags'][0] != None:
                fmt_msg['tags'] = msg['_event_tags']
            else:
                fmt_msg['tags'] = ['Dummy']
        else:
            fmt_msg['tags'] = ['Dummy']
        #fmt_msg['tags'] = msg['_event_tags']
        fmt_msg['route'] = 3
        fmt_msg['fetchTimestamp'] = int(time.time() * 1000)
        fmt_msg['rawdata'] = msg['rawdata']
        return fmt_msg

    def process(self, row):
        if self.process_name == 'dim_inventory_customer_profiles_norm_v0':
            yield self.dim_inventory_customer_profiles_norm_v0(row)
        elif self.process_name == 'vrepair_ticket_opened_norm_v0':
            yield self.vrepair_ticket_opened_norm_v0(row)
        elif self.process_name == 'dom_fact_sls_to_prov_walk' or self.process_name == 'dom_cdi_keys_xref':
            yield self.dom_cdi_keys_xref_dom_fact_sls_to_prov_walk(row)
        elif self.process_name == 'model_scores_prospect_hist':
            yield self.model_scores_prospect_hist(row)
        elif self.process_name == 'wln_model_scores':
            yield self.wln_model_scores(row)        
        else:
            raise ValueError(f"Unknown process_name: {self.process_name}")

class TemplateOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
        parser.add_argument('--query', dest='query', required=True, help='BigQuery SQL query')
        parser.add_argument('--gcs_folder_path', dest='gcs_folder_path', required=True, help='Output GCS folder path')
        parser.add_argument('--process_name', dest='process_name', required=True, help='Output GCS folder path')

def run(argv=None):
    parser = argparse.ArgumentParser()
    known_args, pipeline_args = parser.parse_known_args(argv)
    
    pipeline_options = TemplateOptions(pipeline_args)
    pipeline_options.view_as(StandardOptions).runner = 'DataflowRunner'
    
    with beam.Pipeline(options=pipeline_options) as p:
        headers = {
            'dom_fact_sls_to_prov_walk':"acct_sk,mon,cust_sk,svc_party_sk,acct_estbd_dt,acct_term_dt",
            'dom_cdi_keys_xref':"acct_sk,mon,cust_sk,svc_party_sk,acct_estbd_dt,acct_term_dt",
            'model_scores_prospect_hist':"addr_id,state_cd,model_id,model_version,round_cycle,model_score,model_decile,model_centile,model_segment,model_subsegment,scoring_driver_1,scoring_driver_2,scoring_driver_3,create_date",
            'wln_model_scores': "acct_sk,acct_type_cd,create_date,epsilon_cust_id,insert_ts,model_centile,model_decile,model_id,model_score,model_subsegment,model_version,round_cycle,scoring_driver_1,scoring_driver_2,scoring_driver_3",
            'dim_inventory_customer_profiles_norm_v0':"ont_activation_date,data_circuit_id,circuit_id,video_circuit_id,service_type,address_id,vision_account_id,vision_customer_id,address_type,line_of_business",
            'vrepair_ticket_opened_norm_v0': "telephone_number,trouble_report_num,address_id,chronic_flag,chronic_total,line_id_trimmed,port_associated_service,date_opened,data_circuit_id,video_circuit_id"
        }
        data = p | 'ReadFromBigQuery' >> beam.io.ReadFromBigQuery(
            query=pipeline_options.query,
            use_standard_sql=True
        )
        csv_data = data | 'PrepareData' >> beam.ParDo(PrepareData(pipeline_options.process_name))
        csv_data | 'PrintData' >> beam.Map(lambda element: logging.log(logging.INFO, f'{pipeline_options.gcs_folder_path}/{pipeline_options.process_name}||{headers.get(pipeline_options.process_name)}'))
        csv_data | 'WriteToGCS' >> beam.io.WriteToText(f'{pipeline_options.gcs_folder_path}/{pipeline_options.process_name}', header= headers.get(pipeline_options.process_name),file_name_suffix='.csv')
if __name__ == '__main__':
    run()
