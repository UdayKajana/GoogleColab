import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.io.gcp.spanner import SpannerIO
from google.cloud import spanner
import logging

class ConvertToSpannerMutation(beam.DoFn):
    def process(self, element):
        # Convert BigQuery row to Spanner mutation
        # Modify the column names and types according to your schema
        mutation = spanner.Mutation(
            insert='your_spanner_table',
            columns=('column1', 'column2', 'column3'),
            values=[(
                element['bq_column1'],
                element['bq_column2'],
                element['bq_column3']
            )]
        )
        yield mutation

def run_pipeline(project_id, instance_id, database_id, query):
    """
    Runs the Dataflow pipeline to transfer data from BigQuery to Cloud Spanner.
    
    Args:
        project_id (str): GCP project ID
        instance_id (str): Spanner instance ID
        database_id (str): Spanner database ID
        query (str): BigQuery query to extract data
    """
    
    # Pipeline options
    options = PipelineOptions([
        '--project', project_id,
        '--runner', 'DataflowRunner',
        '--region', 'us-central1',  # modify as needed
        '--temp_location', 'gs://your-bucket/temp',  # modify with your bucket
        '--setup_file', './setup.py'  # if you have dependencies
    ])

    # Define the pipeline
    with beam.Pipeline(options=options) as pipeline:
        # Read from BigQuery
        bq_data = (pipeline 
            | 'ReadFromBigQuery' >> beam.io.ReadFromBigQuery(
                query=query,
                use_standard_sql=True,
                project=project_id)
        )

        # Convert to Spanner mutations
        spanner_mutations = (bq_data 
            | 'ConvertToMutations' >> beam.ParDo(ConvertToSpannerMutation())
        )

        # Write to Cloud Spanner
        _ = (spanner_mutations 
            | 'WriteToSpanner' >> SpannerIO.write(
                project_id=project_id,
                instance_id=instance_id,
                database_id=database_id,
                max_batch_size_bytes=1048576  # 1MB batch size
            )
        )

if __name__ == '__main__':
    # Configuration
    PROJECT_ID = 'your-project-id'
    INSTANCE_ID = 'your-spanner-instance'
    DATABASE_ID = 'your-spanner-database'
    
    # BigQuery query to extract data
    QUERY = """
    SELECT column1, column2, column3
    FROM `project.dataset.table`
    WHERE condition = true
    """

    # Set up logging
    logging.getLogger().setLevel(logging.INFO)
    
    # Run the pipeline
    run_pipeline(PROJECT_ID, INSTANCE_ID, DATABASE_ID, QUERY)
