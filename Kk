import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions
from apache_beam.io.gcp.pubsub import ReadFromPubSub
import fastavro
import argparse
import logging
import datetime
from io import BytesIO

logging.getLogger().setLevel(logging.ERROR)

class TemplateOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
        parser.add_value_provider_argument('--params', type=str)

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--project', required=True, help='GCP Project ID')
    parser.add_argument('--runner', default="DataflowRunner", help='Pipeline runner')
    parser.add_argument('--region', default="us-east4", help='GCP project region')
    parser.add_argument('--staging_location', required=True, help='Staging GCS bucket path')
    parser.add_argument('--temp_location', required=True, help='Temp GCS bucket path')
    parser.add_argument('--template_location', required=True, help='GCS bucket path to save template')
    parser.add_argument('--input_sub', required=True, help='Input Subscription')
    return parser.parse_known_args()

class DecodeDoFn(beam.DoFn):
    SCHEMA = {
        "namespace": "com.vz.vznet",
        "type": "record",
        "name": "VznetDefault",
        "doc": "Default schema for events in transit",
        "fields": [
            {"name": "timestamp", "type": "long"},
            {"name": "host", "type": "string"},
            {"name": "src", "type": "string"},
            {"name": "_event_ingress_ts", "type": "long"},
            {"name": "_event_origin", "type": "string"},
            {"name": "_event_tags", "type": {"type": "array", "items": "string"}},
            {"name": "_event_route", "type": "string"},
            {"name": "_event_metrics", "type": ["null", "bytes"], "default": None},
            {"name": "rawdata", "type": "bytes"}
        ]
    }

    def process(self, element):
        try:
            bytes_reader = BytesIO(element)
            avro_reader = fastavro.reader(bytes_reader, self.SCHEMA)
            message = next(avro_reader)

            # Decode bytes fields
            message['rawdata'] = message['rawdata'].decode("utf-8")
            if message['_event_metrics'] is not None:
                message['_event_metrics'] = message['_event_metrics'].decode("utf-8")

            # Reformat message
            reformatted = {
                'timestamp': message['timestamp'],
                'host': message['host'],
                'src': message['src'],
                'ingressTimestamp': message['_event_ingress_ts'],
                'origins': [message['_event_origin']],
                'tags': message['_event_tags'],
                'route': 3,
                'fetchTimestamp': datetime.datetime.now(),
                'rawdata': message['rawdata']
            }
            yield reformatted
        except Exception as e:
            logging.error(f'Error deserializing the records: {e}')

def process_avro_to_csv(message_data):
    try:
        bytes_reader = BytesIO(message_data)
        avro_reader = fastavro.reader(bytes_reader)
        records = [record for record in avro_reader]
        
        if not records:
            logging.warning("No records found in Avro message")
            return None

        import csv
        import io
        import uuid

        fieldnames = list(records[0].keys())
        csv_buffer = io.StringIO()
        writer = csv.DictWriter(csv_buffer, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(records)

        filename = f"message_{uuid.uuid4()}.csv"
        return filename, csv_buffer.getvalue()
    except Exception as e:
        logging.error(f"Error processing Avro message: {e}")
        return None

def write_csv_to_gcs(element, gcs_location):
    from apache_beam.io.gcp.gcsio import GcsIO
    
    try:
        filename, csv_data = element
        if not csv_data:
            logging.warning(f"No CSV data to write for {filename}")
            return

        gcs_path = f"{gcs_location}/{filename}"
        gcs_io = GcsIO()
        with gcs_io.open(gcs_path, 'w') as gcs_file:
            gcs_file.write(csv_data.encode("utf-8"))
        
        logging.info(f"Written to GCS: {gcs_path}")
    except Exception as e:
        logging.error(f"Error writing to GCS: {e}")

def run_pipeline():
    known_args, beam_args = parse_args()
    
    options = {
        'project': known_args.project,
        'runner': known_args.runner,
        'region': known_args.region,
        'staging_location': known_args.staging_location,
        'temp_location': known_args.temp_location,
        'template_location': known_args.template_location
    }
    
    pipeline_options = PipelineOptions.from_dictionary(options)
    pipeline_options.view_as(StandardOptions).streaming = True
    
    with beam.Pipeline(options=pipeline_options) as p:
        pubsub_data = (p 
            | 'ReadFromPubsub' >> ReadFromPubSub(
                subscription=f"projects/{known_args.project}/subscriptions/{known_args.input_sub}")
            | "PrintData" >> beam.Map(lambda element: logging.log(logging.INFO, str(element)))
            | "Decode" >> beam.ParDo(DecodeDoFn()))

if __name__ == "__main__":
    run_pipeline()
