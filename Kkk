import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
import logging
import argparse

class BQToGCSOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
        # Input BigQuery table
        parser.add_value_provider_argument(
            '--input_table',
            type=str,
            help='Input BigQuery table in format: project_id:dataset_id.table_id'
        )
        
        # Output GCS path
        parser.add_value_provider_argument(
            '--output_path',
            type=str,
            help='Output GCS path (e.g., gs://bucket/path/prefix)'
        )

def run(argv=None):
    """Main entry point for the Dataflow pipeline."""
    parser = argparse.ArgumentParser()
    known_args, pipeline_args = parser.parse_known_args(argv)

    # Create pipeline options
    pipeline_options = BQToGCSOptions(pipeline_args)

    with beam.Pipeline(options=pipeline_options) as pipeline:
        # Read from BigQuery using ValueProvider
        data = pipeline | 'ReadFromBigQuery' >> beam.io.ReadFromBigQuery(
            table=pipeline_options.input_table,
            use_standard_sql=True
        )

        # Convert each row to CSV format
        def row_to_csv(row):
            return ','.join(str(value) for value in row.values())

        csv_data = data | 'ConvertToCSV' >> beam.Map(row_to_csv)

        # Write to GCS using ValueProvider
        _ = csv_data | 'WriteToGCS' >> beam.io.WriteToText(
            file_path_prefix=pipeline_options.output_path,
            file_name_suffix='.csv',
            num_shards=1
        )

if __name__ == '__main__':
    logging.getLogger().setLevel(logging.INFO)
    run()



python bq_to_gcs_template.py \

    --runner DataflowRunner \

    --project [YOUR_PROJECT_ID] \

    --staging_location gs://[BUCKET_NAME]/staging \

    --temp_location gs://[BUCKET_NAME]/temp \

    --template_location gs://[BUCKET_NAME]/templates/bq_to_gcs \

    --region [YOUR_REGION]


gcloud dataflow jobs run bq-to-gcs-job \

    --gcs-location gs://[BUCKET_NAME]/templates/bq_to_gcs \

    --region [YOUR_REGION] \

    --parameters \

input_table="project_id:dataset_id.table_id",\

output_path="gs://bucket-name/path/to/output"
