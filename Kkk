import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from fastavro import reader
import io
import csv
import typing
import uuid

# Define your Pub/Sub subscription and GCS bucket details
project_id = "vz-it-np-gudv-dev-vzntdo-0"
subscription_id = "wireline_churn_test_topic-sub"
subscription_path = f"projects/{project_id}/subscriptions/{subscription_id}"
gcs_bucket = "vznet-test"
gcs_output_path = "wireline_churn_test/tgt/customer_profile"

# Avro schema
schema = [
    "ont_activation_date",
    "data_circuit_id",
    "circuit_id",
    "video_circuit_id",
    "service_type",
    "address_id",
    "vision_account_id",
    "vision_customer_id",
    "address_type",
    "line_of_business"
]

# Function to parse Avro messages
def parse_avro_message(message):
    try:
        avro_bytes = message.data
        avro_file = io.BytesIO(avro_bytes)
        avro_reader = reader(avro_file)
        
        for record in avro_reader:
            yield record  # Yield each record individually

    except Exception as e:
        print(f"Error parsing Avro message: {e}")

# Function to format a single record into CSV
def format_to_csv(record: dict) -> str:
    output = io.StringIO()
    writer = csv.DictWriter(output, fieldnames=schema)
    writer.writeheader()  # Write header for the CSV
    writer.writerow(record)  # Write the single record
    return output.getvalue()

# Function to create a unique filename for each message
def create_unique_filename(record: dict) -> typing.Tuple[str, str]:
    csv_data = format_to_csv(record)
    # Generate a unique filename using a UUID
    filename = f"customer_profile_{uuid.uuid4()}.csv"
    return filename, csv_data

# Define pipeline options
pipeline_options = PipelineOptions(
    streaming=True,
    project=project_id,
    temp_location=f"gs://{gcs_bucket}/wireline_churn_test/tmp",
    region="us-central1"
)

def run_pipeline():
    with beam.Pipeline(options=pipeline_options) as pipeline:
        messages = (
            pipeline
            | "ReadFromPubSub" >> beam.io.ReadFromPubSub(subscription=subscription_path)
        )

        parsed_data = (
            messages
            | "ParseAvroMessages" >> beam.FlatMap(parse_avro_message)
        )

        csv_data = (
            parsed_data
            | "FormatToCSV" >> beam.Map(create_unique_filename)  # Format and create unique filenames
        )

        # Step to write each CSV data to a unique file in GCS
        csv_data | "WriteToGCS" >> beam.Map(lambda x: beam.io.WriteToText(
            f"gs://{gcs_bucket}/{gcs_output_path}/{x[0]}",
            file_name_suffix=".csv",
            num_shards=1  # Control number of shards if desired
        ))

if __name__ == '__main__':
    run_pipeline()

def parse_avro_message(message):
    try:
        avro_bytes = message.data
        avro_file = io.BytesIO(avro_bytes)
        avro_reader = reader(avro_file)
        
        records = []
        for record in avro_reader:
            records.append(record)  # Accumulate records into a list
        
        yield records  # Yield the entire batch of records
    except Exception as e:
        print(f"Error parsing Avro message: {e}")

def format_to_csv(records: typing.List[dict]) -> str:
    output = io.StringIO()
    writer = csv.DictWriter(output, fieldnames=schema)

    writer.writeheader()  # Write header for the CSV
    for record in records:
        writer.writerow(record)  # Write each record

    return output.getvalue()

def create_unique_filename(records: typing.List[dict]) -> typing.Tuple[str, str]:
    csv_data = format_to_csv(records)

    # Generate a unique filename using a UUID
    filename = f"customer_profile_{uuid.uuid4()}.csv"

    return filename, csv_data

def run_pipeline():
    with beam.Pipeline(options=pipeline_options) as pipeline:
        messages = (
            pipeline
            | "ReadFromPubSub" >> beam.io.ReadFromPubSub(subscription=subscription_path)
        )

        parsed_data = (
            messages
            | "ParseAvroMessages" >> beam.FlatMap(parse_avro_message)
        )

        csv_data = (
            parsed_data
            | "FormatToCSV" >> beam.Map(create_unique_filename)  # Format and create unique filenames
        )

        # Step to write CSV data to a unique file in GCS
        csv_data | "WriteToGCS" >> beam.Map(lambda x: beam.io.WriteToText(
            f"gs://{gcs_bucket}/{gcs_output_path}/{x[0]}",
            file_name_suffix=".csv",
            num_shards=1  # Control number of shards if desired
        ))

if __name__ == '__main__':
    run_pipeline()

