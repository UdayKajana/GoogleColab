import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from fastavro import reader
import io
import csv
import typing
import logging
import json
import uuid

# Configure logging
logging.getLogger().setLevel(logging.INFO)

# Define project and subscription details
project_id = "vz-it-np-gudv-dev-vzntdo-0"
subscription_id = "wireline_churn_test_topic-sub"
subscription_path = f"projects/{project_id}/subscriptions/{subscription_id}"
gcs_bucket = "vznet-test"
gcs_output_path = "wireline_churn_test/tgt/customer_profile"

# Define the schema for the CSV
schema = [
    "ont_activation_date",
    "data_circuit_id",
    "circuit_id",
    "video_circuit_id",
    "service_type",
    "address_id",
    "vision_account_id",
    "vision_customer_id",
    "address_type",
    "line_of_business"
]

# Function to parse Avro messages
def parse_avro_message(message):
    logging.info("Parsing message...")
    try:
        avro_file = io.BytesIO(message)
        avro_reader = reader(avro_file)

        for record in avro_reader:
            yield record  # Yield each record directly
    except Exception as e:
        logging.error(f"Error parsing Avro message: {e}")

# Function to transform records to CSV-friendly format
class TransformToCSV(beam.DoFn):
    def process(self, element):
        flat_records = {}
        for key, value in element.items():
            if isinstance(value, (dict, list)):
                flat_records[key] = json.dumps(value)
            else:
                flat_records[key] = value
        yield flat_records  # Yield the modified record

def run_pipeline():
    pipeline_options = PipelineOptions(
        streaming=True,
        project=project_id,
        temp_location=f"gs://{gcs_bucket}/wireline_churn_test/tmp",
        region="us-central1"
    )

    with beam.Pipeline(options=pipeline_options) as pipeline:
        messages = (pipeline 
                    | "ReadFromPubSub" >> beam.io.ReadFromPubSub(subscription=subscription_path))
        
        parsed_data = (messages 
                       | "ParseAvroMessages" >> beam.FlatMap(parse_avro_message))
        
        csv_data = (parsed_data 
                    | "TransformToCSV" >> beam.ParDo(TransformToCSV()))
        
        # Group all records into a single element for writing
        grouped_data = (csv_data 
                        | "DummyKey" >> beam.Map(lambda record: (None, record))  # Add a dummy key
                        | "GroupAll" >> beam.GroupByKey()  # Group by the dummy key
                        | "Flatten" >> beam.FlatMap(lambda kv: kv[1])  # Flatten the grouped result
                       )

        # Write to a single CSV file
        (grouped_data 
         | "WriteToGCS" >> 
         beam.io.WriteToText(
             f"gs://{gcs_bucket}/{gcs_output_path}/output",  # Specify the path for the output
             file_name_suffix=".csv",
             shard_name_template=""  # This ensures a single output file
         ))

        # Optional: Print elements for debugging
        print_data = (csv_data | "Print" >> beam.Map(Print_element))

def Print_element(element):
    logging.info("element=" + str(element))
    return element

if __name__ == '__main__':
    run_pipeline()

