import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
import logging
import argparse

class BQToGCSOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
        # Input BigQuery table
        parser.add_value_provider_argument(
            '--input_table',
            type=str,
            help='Input BigQuery table in format: project_id:dataset_id.table_id'
        )
        
        # Output GCS path
        parser.add_value_provider_argument(
            '--output_path',
            type=str,
            help='Output GCS path (e.g., gs://bucket/path/prefix)'
        )

def run(argv=None):
    parser = argparse.ArgumentParser()
    known_args, pipeline_args = parser.parse_known_args(argv)

    # Create pipeline options
    pipeline_options = BQToGCSOptions(pipeline_args)

    with beam.Pipeline(options=pipeline_options) as pipeline:
        # Read from BigQuery using ValueProvider
        data = pipeline | 'ReadFromBigQuery' >> beam.io.ReadFromBigQuery(
            table=pipeline_options.input_table,
            use_standard_sql=True
        )

        # Convert each row to CSV format
        def row_to_csv(row):
            return ','.join(str(value) for value in row.values())

        csv_data = data | 'ConvertToCSV' >> beam.Map(row_to_csv)

        # Write to GCS using ValueProvider
        _ = csv_data | 'WriteToGCS' >> beam.io.WriteToText(
            file_path_prefix=pipeline_options.output_path,
            file_name_suffix='.csv',
            num_shards=1
        )

if __name__ == '__main__':
    logging.getLogger().setLevel(logging.INFO)
    run()


python3 /vzwhome/udayka/workspace/flex_template_work/pipeline.py \
--project vz-it-np-gudv-dev-vzntdo-0 \
--runner DataflowRunner \
--region us-east4 --staging_location gs://vznet-test/wireline_churn_bq_spanner/stg \
--temp_location gs://vznet-test/wireline_churn_test/tmp/ \
--template_location gs://vznet-test/wireline_churn_test/code/pipeline \
--input_table test_dataset.dim_inventory_customer_profiles_norm_v10 \
--output_path gs://vznet-test/wireline_churn_test/stg11



gcloud dataflow jobs run wireline_churn_bq_spanner1018 \
--gcs-location gs://vznet-test/wireline_churn_test/code/pipeline \
--region us-east4 --num-workers 2 --max-workers 2 --worker-machine-type n2-standard-2 \
--disable-public-ips --network=shared-np-east \
--dataflow-kms-key=projects/vz-it-np-d0sv-vsadkms-0/locations/us-east4/keyRings/vz-it-np-kr-aid/cryptoKeys/vz-it-np-kms-gudv \
--subnetwork=https://www.googleapis.com/compute/v1/projects/vz-it-np-exhv-sharedvpc-228116/regions/us-east4/subnetworks/shared-np-east-green-subnet-2 \
--service-account-email sa-dev-gudv-app-vzntdo-0@vz-it-np-gudv-dev-vzntdo-0.iam.gserviceaccount.com \
--parameters input_table="test_dataset.dim_inventory_customer_profiles_norm_v0", output_path="gs://vznet-test/wireline_churn_test/stg"

getting ERROR: (gcloud.dataflow.jobs.run) unrecognized arguments: output_path=gs://vznet-test/wireline_churn_test/stg

To search the help text of gcloud commands, run:
  gcloud help -- SEARCH_TERMS

