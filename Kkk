import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
import argparse
import logging

logging.getLogger().setLevel(logging.INFO)

class BQToGCSOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
        # Input BigQuery table
        parser.add_value_provider_argument(
            '--input_table',
            type=str,
            help='Input BigQuery table in format: project_id:dataset_id.table_id'
        )
        
        # Output GCS path
        parser.add_value_provider_argument(
            '--output_path',
            type=str,
            help='Output GCS path (e.g., gs://bucket/path/prefix)'
        )

def run(argv=None):
    parser = argparse.ArgumentParser()
    known_args, pipeline_args = parser.parse_known_args(argv)
    pipeline_options = BQToGCSOptions(pipeline_args)

    # Create your pipeline
    with beam.Pipeline(options=pipeline_options) as pipeline:
        # Read data from BigQuery using the parameter
        data = pipeline | 'ReadFromBigQuery' >> beam.io.ReadFromBigQuery(
            query=lambda: f"SELECT * FROM `{pipeline_options.input_table.get()}`",
            use_standard_sql=True,
        )

        # Convert to CSV
        csv_data = data | 'ConvertToCSV' >> beam.Map(lambda row: ','.join(str(x) for x in row.values()))

        # Add logging step
        def print_element(element, input_table, output_path):
            logging.info(f"Input table: {input_table}")
            logging.info(f"Output path: {output_path}")
            return element

        print_data = csv_data | "PrintData" >> beam.Map(
            lambda element: print_element(
                element,
                pipeline_options.input_table.get(),
                pipeline_options.output_path.get()
            )
        )

        # Write to GCS using the parameter
        csv_data | 'WriteToGCS' >> beam.io.WriteToText(
            file_path_prefix=pipeline_options.output_path,
            file_name_suffix='.csv',
            header="ont_activation_date,data_circuit_id,circuit_id,video_circuit_id,service_type,address_id,vision_account_id,vision_customer_id,address_type,line_of_business"
        )

if __name__ == '__main__':
    logging.getLogger().setLevel(logging.INFO)
    run()



# Create template
python3 /vzwhome/udayka/workspace/wireline_churn_bq_csv.py \
--project vz-it-np-gudv-dev-vzntdo-0 \
--runner DataflowRunner \
--region us-east4 \
--staging_location gs://vznet-test/wireline_churn_bq_spanner/stg \
--temp_location gs://vznet-test/wireline_churn_test/tmp/ \
--template_location gs://vznet-test/wireline_churn_test/code/wireline_churn_bq_csv

# Run the job
gcloud dataflow jobs run wireline_churn_bq_spanner1018 \
--gcs-location gs://vznet-test/wireline_churn_test/code/wireline_churn_bq_csv \
--region us-east4 \
--num-workers 2 \
--max-workers 2 \
--worker-machine-type n2-standard-2 \
--disable-public-ips \
--network=shared-np-east \
--dataflow-kms-key=projects/vz-it-np-d0sv-vsadkms-0/locations/us-east4/keyRings/vz-it-np-kr-aid/cryptoKeys/vz-it-np-kms-gudv \
--subnetwork=https://www.googleapis.com/compute/v1/projects/vz-it-np-exhv-sharedvpc-228116/regions/us-east4/subnetworks/shared-np-east-green-subnet-2 \
--service-account-email sa-dev-gudv-app-vzntdo-0@vz-it-np-gudv-dev-vzntdo-0.iam.gserviceaccount.com \
--parameters \
input_table=vz-it-np-gudv-dev-vzntdo-0.test_dataset.dim_inventory_customer_profiles_norm_v0,\
output_path=gs://vznet-test/wireline_churn_test/stg/output
