import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.transforms.window import FixedWindows
from fastavro import reader
import io
import csv

# Define your Pub/Sub subscription and GCS bucket details
project_id = "vz-it-np-gudv-dev-vzntdo-0"
subscription_id = "wireline_churn_test_topic-sub"
subscription_path = f"projects/{project_id}/subscriptions/{subscription_id}"
gcs_bucket = "vznet-test"
gcs_file_path = "wireline_churn_test/tgt/customer_profile"

# Avro schema
schema = {
    "namespace": "example.avro",
    "type": "record",
    "name": "customerprofile",
    "fields": [
        {"name": "ont_activation_date", "type": ["string", "null"]},
        {"name": "data_circuit_id", "type": ["string", "null"]},
        {"name": "circuit_id", "type": ["string", "null"]},
        {"name": "video_circuit_id", "type": ["string", "null"]},
        {"name": "service_type", "type": ["string", "null"]},
        {"name": "address_id", "type": "long"},
        {"name": "vision_account_id", "type": ["string", "null"]},
        {"name": "vision_customer_id", "type": ["string", "null"]},
        {"name": "address_type", "type": ["string", "null"]},
        {"name": "line_of_business", "type": ["string", "null"]}
    ]
}

# Function to parse Avro messages
def parse_avro_message(message):
    avro_bytes = message.data
    avro_file = io.BytesIO(avro_bytes)
    avro_reader = reader(avro_file)
    for record in avro_reader:
        yield (record["address_id"], record)  # Use address_id as a key for grouping

# Function to format grouped records as CSV
def format_csv_lines(grouped_records):
    key, records = grouped_records
    output = io.StringIO()
    writer = csv.DictWriter(output, fieldnames=[field['name'] for field in schema['fields']])
    writer.writeheader()
    for record in records:
        writer.writerow(record)
    return output.getvalue()

# Define the Beam pipeline
pipeline_options = PipelineOptions(
    streaming=True,  # Enable streaming mode
    project=project_id,
    temp_location=f"gs://{gcs_bucket}/temp"
)

with beam.Pipeline(options=pipeline_options) as pipeline:
    # Step 1: Read messages from Pub/Sub
    messages = (
        pipeline
        | "ReadFromPubSub" >> beam.io.ReadFromPubSub(subscription=subscription_path)
    )

    # Step 2: Apply Fixed Windowing
    windowed_data = (
        messages
        | "WindowIntoFixed" >> beam.WindowInto(FixedWindows(60))  # 1-minute windows
    )

    # Step 3: Parse Avro messages and assign keys
    parsed_data = (
        windowed_data
        | "ParseAvroMessages" >> beam.FlatMap(parse_avro_message)  # (key, record)
    )

    # Step 4: Group by Key within the window
    grouped_data = (
        parsed_data
        | "GroupByKey" >> beam.GroupByKey()  # Group records by key
    )

    # Step 5: Format grouped records as CSV
    csv_lines = (
        grouped_data
        | "FormatAsCSV" >> beam.Map(format_csv_lines)  # Returns CSV-formatted string
    )

    # Step 6: Write CSV lines to GCS
    csv_lines | "WriteToGCS" >> beam.io.WriteToText(
        f"gs://{gcs_bucket}/{gcs_file_path}",
        file_name_suffix=".csv",
        shard_name_template="",  # Avoid file sharding
    )
