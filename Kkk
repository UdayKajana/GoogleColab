import apache_beam as beam
from apache_beam.io.avroio import ReadFromAvro
from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions, GoogleCloudOptions, SetupOptions
import csv
import io
import uuid
from google.cloud import storage

class AvroToCsvPipeline:
    def __init__(self):
        """
        Initialize the Avro to CSV conversion pipeline with hardcoded configurations.
        """
        # Hardcoded configuration parameters
        self.PROJECT_ID = 'your-project-id'  # Replace with your GCP project ID
        self.REGION = 'us-central1'  # GCP region for Dataflow job
        
        # Input Avro file configuration
        self.INPUT_AVRO_PATH = 'gs://your-input-bucket/input.avro'  # Full path to input Avro file
        
        # Output CSV configuration
        self.OUTPUT_BUCKET = 'your-output-bucket'  # GCS bucket for output
        self.OUTPUT_PATH = 'avro_to_csv_output/'  # Path within the bucket
        
        # Staging and temp locations
        self.STAGING_LOCATION = f'gs://{self.OUTPUT_BUCKET}/staging'
        self.TEMP_LOCATION = f'gs://{self.OUTPUT_BUCKET}/temp'
        
        # Pipeline runner (DataflowRunner for cloud execution)
        self.RUNNER = 'DataflowRunner'
        
        # Configure pipeline options
        self.pipeline_options = self.configure_pipeline_options()
    
    def configure_pipeline_options(self):
        """
        Configure comprehensive pipeline options for Dataflow with hardcoded settings.
        
        Returns:
            PipelineOptions: Configured pipeline options
        """
        # Create pipeline options
        options = PipelineOptions()
        
        # Set Google Cloud specific configurations
        google_cloud_options = options.view_as(GoogleCloudOptions)
        google_cloud_options.project = self.PROJECT_ID
        google_cloud_options.region = self.REGION
        google_cloud_options.staging_location = self.STAGING_LOCATION
        google_cloud_options.temp_location = self.TEMP_LOCATION
        
        # Generate a unique job name
        google_cloud_options.job_name = f'avro-to-csv-{uuid.uuid4().hex[:10]}'
        
        # Set runner
        options.view_as(StandardOptions).runner = self.RUNNER
        
        # Configure setup options
        setup_options = options.view_as(SetupOptions)
        setup_options.save_main_session = True
        
        return options
    
    def convert_to_csv(self, element):
        """
        Convert a single Avro record to a CSV string.
        
        Args:
            element (dict): A single Avro record
        
        Returns:
            tuple: (output_filename, csv_content)
        """
        # Generate a unique filename for each record
        unique_id = str(uuid.uuid4())
        output_filename = f'record_{unique_id}.csv'
        
        # Convert the record to a CSV string
        output = io.StringIO()
        writer = csv.writer(output)
        
        # Write headers
        writer.writerow(element.keys())
        
        # Write data
        writer.writerow(element.values())
        
        return (output_filename, output.getvalue())
    
    def upload_to_gcs(self, record):
        """
        Upload a single CSV record to GCS.
        
        Args:
            record (tuple): (filename, file_content)
        """
        filename, content = record
        storage_client = storage.Client()
        bucket = storage_client.bucket(self.OUTPUT_BUCKET)
        blob = bucket.blob(f"{self.OUTPUT_PATH}{filename}")
        
        blob.upload_from_string(content)
        print(f"File {filename} uploaded to {self.OUTPUT_BUCKET}")
    
    def run(self):
        """
        Execute the Avro to CSV conversion pipeline.
        """
        with beam.Pipeline(options=self.pipeline_options) as p:
            # Read Avro file
            records = p | 'Read Avro' >> ReadFromAvro(self.INPUT_AVRO_PATH)
            
            # Convert records to CSV
            csv_records = records | 'Convert to CSV' >> beam.Map(self.convert_to_csv)
            
            # Upload each CSV to GCS
            csv_records | 'Upload to GCS' >> beam.Map(self.upload_to_gcs)

def main():
    """
    Main entry point for the Avro to CSV Dataflow pipeline.
    """
    pipeline = AvroToCsvPipeline()
    pipeline.run()

if __name__ == '__main__':
    main()

# Pre-Execution Checklist:
# 1. Replace 'your-project-id' with actual GCP project ID
# 2. Update INPUT_AVRO_PATH with full GCS path to input Avro file
# 3. Set OUTPUT_BUCKET to your target GCS bucket
# 4. Ensure you have necessary GCP permissions
# 5. Authenticate with GCP before running
