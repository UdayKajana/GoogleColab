import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.options.pipeline_options import StandardOptions
from apache_beam.options.pipeline_options import GoogleCloudOptions
from apache_beam.io.gcp.bigquery import ReadFromBigQuery

def run():
    # Define pipeline options
    pipeline_options = PipelineOptions()

    # Validate Google Cloud options
    google_cloud_options = pipeline_options.view_as(GoogleCloudOptions)
    if not google_cloud_options.project:
        raise ValueError("Project must be specified.")
    if not google_cloud_options.temp_location:
        raise ValueError("Temporary GCS location must be specified.")
    if not google_cloud_options.staging_location:
        raise ValueError("Staging GCS location must be specified.")

    # Set runner (DataflowRunner for cloud execution)
    pipeline_options.view_as(StandardOptions).runner = 'DataflowRunner'

    # Debug logs for verification
    print(f"Project: {google_cloud_options.project}")
    print(f"Region: {google_cloud_options.region}")
    print(f"Temp Location: {google_cloud_options.temp_location}")
    print(f"Staging Location: {google_cloud_options.staging_location}")

    # Create pipeline
    with beam.Pipeline(options=pipeline_options) as pipeline:
        # Read query from pipeline options
        query = pipeline_options.view_as(GoogleCloudOptions).query
        if not query:
            raise ValueError("Query must be specified.")

        # Read from BigQuery using the provided query
        records = pipeline | 'ReadFromBigQuery' >> ReadFromBigQuery(
            query=query
            use_standard_sql=True,
            gcs_location=google_cloud_options.temp_location
        )

        # Add your transformations here (example placeholder)
        transformed_records = records | 'ExampleTransform' >> beam.Map(lambda record: record)

        # Write transformed records or add further processing if needed

if __name__ == '__main__':
    run()

{
    "name": "Bigquery to GCS Pipeline",
    "description": "A template for reading from BigQuery and processing data",
    "parameters": [
        {
            "name": "query",
            "label": "BigQuery SQL query",
            "helpText": "The SQL query to execute",
            "isOptional": false,
            "regexes": [],
            "paramType": "TEXT"
        }
    ],
    "defaultEnvironment": {
        "tempLocation": "gs://vznet-test/wireline_churn_test/tmp/",
        "network": "shared-np-east",
        "subnetwork": "https://www.googleapis.com/compute/v1/projects/vz-it-np-exhv-sharedvpc-228116/regions/us-east4/subnetworks/shared-np-east-green-subnet-2",
        "serviceAccountEmail": "sa-dev-gudv-app-vzntdo-0@vz-it-np-gudv-dev-vzntdo-0.iam.gserviceaccount.com",
        "project": "vz-it-np-gudv-dev-vzntdo-0"
    }
}

gcloud builds submit --tag us-east4-docker.pkg.dev/vz-it-np-gudv-dev-vzntdo-0/vznet/pipeline_env:V1 /workspace
gcloud dataflow flex-template build gs://vznet-test/wireline_churn_test/src/template.json --image us-east4-docker.pkg.dev/vz-it-np-gudv-dev-vzntdo-0/vznet/pipeline_env:V1 --sdk-language "PYTHON" --metadata-file metadata.json
gcloud dataflow flex-template run bigquery123 --template-file-gcs-location gs://vznet-test/wireline_churn_test/src/template.json --project vz-it-np-gudv-dev-vzntdo-0 --region us-east4 --parameters query="SELECT * FROM vz-it-np-gudv-dev-vzntdo-0.vzn_nsdl_common_core_tbls.echo_ticket_opened_proc_v0 LIMIT 10" --temp-location gs://vznet-test/wireline_churn_test/tmp/ --num-workers 2 --max-workers 2 --worker-machine-type n2-standard-2 --disable-public-ips --network=shared-np-east --dataflow-kms-key=projects/vz-it-np-d0sv-vsadkms-0/locations/us-east4/keyRings/vz-it-np-kr-aid/cryptoKeys/vz-it-np-kms-gudv --subnetwork=https://www.googleapis.com/compute/v1/projects/vz-it-np-exhv-sharedvpc-228116/regions/us-east4/subnetworks/shared-np-east-green-subnet-2 --service-account-email sa-dev-gudv-app-vzntdo-0@vz-it-np-gudv-dev-vzntdo-0.iam.gserviceaccount.com
