import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
import logging
import argparse

logging.getLogger().setLevel(logging.INFO)

class PubSubLoggingOptions(PipelineOptions):
    @classmethod
    def add_argparse_args(cls, parser):
        parser.add_value_provider_argument(
            '--input_subscription',
            type=str,
            help='Pub/Sub subscription path'
        )

def subscribe_PubSub_topic(pipeline, input_subscription_path):
    """
    Subscribes to a Pub/Sub topic and logs the received messages.

    Args:
        pipeline: The Apache Beam pipeline instance.
        input_subscription_path: The Pub/Sub subscription path (as a RuntimeValueProvider).

    Returns:
        A PCollection of messages consumed from Pub/Sub.
    """
    return (
        pipeline
        | "ReadFromPubSub" >> beam.io.ReadFromPubSub(subscription=input_subscription_path)
        | "LogMessages" >> beam.Map(lambda message: logging.info(f"Received message: {message.decode('utf-8')}"))
    )

def run_pipeline(argv=None):
    parser = argparse.ArgumentParser()
    known_args, pipeline_args = parser.parse_known_args(argv)
    pipeline_options = PubSubLoggingOptions(pipeline_args)

    with beam.Pipeline(options=pipeline_options) as pipeline:
        # Subscribe to Pub/Sub and log messages
        _ = subscribe_PubSub_topic(pipeline, pipeline_options.input_subscription)

if __name__ == '__main__':
    run_pipeline()


python pipeline.py \
    --runner DataflowRunner \
    --project <your-gcp-project-id> \
    --region <your-region> \
    --staging_location gs://<your-bucket>/staging \
    --temp_location gs://<your-bucket>/temp \
    --template_location gs://<your-bucket>/templates/pubsub_logging_template

gcloud dataflow jobs run pubsub-logging-job \
    --gcs-location gs://<your-bucket>/templates/pubsub_logging_template \
    --region <your-region> \
    --parameters input_subscription=projects/<your-project-id>/subscriptions/<new-subscription-id>
