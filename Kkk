import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
import io
import csv
import logging
import uuid
import argparse

logging.getLogger().setLevel(logging.INFO)

global pipeline_options

class BQToGCSOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
        parser.add_value_provider_argument(
            '--input_subscription',
            type=str,
            help='PubSub subscription path (e.g., projects/YOUR_PROJECT_ID/subscriptions/YOUR_SUBSCRIPTION_NAME)'
        )
        parser.add_value_provider_argument(
            '--output_path',
            type=str,
            help='Output GCS path (e.g., gs://bucket/path/prefix)'
        )

def process_avro_to_csv(message_data):
    logging.log(logging.INFO, str(message_data))
    from io import BytesIO
    import fastavro
    import csv
    import uuid

    try:
        # Read Avro data
        bytes_reader = BytesIO(message_data)
        avro_reader = fastavro.reader(bytes_reader)

        # Extract records
        records = [record for record in avro_reader]

        if not records:
            logging.warning("No records found in the Avro message.")
            return None

        # Convert records to CSV format
        fieldnames = list(records[0].keys())
        csv_buffer = io.StringIO()
        writer = csv.DictWriter(csv_buffer, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(records)

        # Generate a unique filename
        unique_id = uuid.uuid4()
        filename = f"message_{unique_id}.csv"
        logging.info(f"Processed message into file: {filename}")

        return filename, csv_buffer.getvalue()

    except Exception as e:
        logging.error(f"Error processing Avro message: {e}")
        return None

def write_csv_to_gcs(element):
    global pipeline_options
    from apache_beam.io.gcp.gcsio import GcsIO

    try:
        filename, csv_data = element

        if not csv_data:
            logging.warning(f"No CSV data to write for {filename}. Skipping.")
            return

        # Write CSV data to GCS
        gcs_io = GcsIO()
        with gcs_io.open(pipeline_options.output_path.get(), 'w') as gcs_file:
            gcs_file.write(csv_data.encode("utf-8"))

        logging.info(f"Successfully wrote {filename} to GCS: {pipeline_options.output_path.get()}")

    except Exception as e:
        logging.error(f"Error writing CSV to GCS: {e}")

# Create a DoFn to extract the subscription string
class ExtractSubscription(beam.DoFn):
    def process(self, element):
        # Yield the subscription string
        yield element

def run_pipeline(argv=None):
    global pipeline_options
    parser = argparse.ArgumentParser()
    known_args, pipeline_args = parser.parse_known_args(argv)
    pipeline_options = BQToGCSOptions(pipeline_args)

    # Start the pipeline
    with beam.Pipeline(options=pipeline_options) as pipeline:
        # Create a single element PCollection with the input subscription
        subscription = (
            pipeline
            | "CreateInputSubscription" >> beam.Create([pipeline_options.input_subscription])
        )
        
        # Extract the subscription
        subscription = (
            subscription
            | "ExtractSubscription" >> beam.ParDo(ExtractSubscription())
        )

        # Use the subscription in the ReadFromPubSub task
        messages = (
            subscription
            | "ReadFromPubSub" >> beam.io.ReadFromPubSub(subscription=beam.pvalue.AsSingleton(subscription))
        )

        csv_files = (
            messages
            | "ProcessAvroToCSV" >> beam.Map(process_avro_to_csv)
        )

        _ = (
            csv_files
            | "WriteCSVToGCS" >> beam.Map(write_csv_to_gcs)
        )

if __name__ == '__main__':
    run_pipeline()

