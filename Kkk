import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
import argparse
import logging
logging.getLogger().setLevel(logging.INFO)
class BQToGCSOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
        parser.add_value_provider_argument(
            '--input_table',
            type=str,
            help='Input BigQuery table in format: project_id.dataset_id.table_id'
        )
        parser.add_value_provider_argument(
            '--spanner_instance_id',
            type=str,
            help='Spanner instance'
        )
        parser.add_value_provider_argument(
            '--spanner_database_id',
            type=str,
            help='Spanner database'
        )
        parser.add_value_provider_argument(
            '--spanner_table_id',
            type=str,
            help='Spanner table'
        )

def run(argv=None):
    parser = argparse.ArgumentParser()
    known_args, pipeline_args = parser.parse_known_args(argv)
    pipeline_options = BQToGCSOptions(pipeline_args)
    with beam.Pipeline(options=pipeline_options) as pipeline:
        data = pipeline | "ReadFromBigQuery" >> beam.io.ReadFromBigQuery(
            query=f'SELECT * FROM {pipeline_options.input_table.get()}', use_standard_sql=True
        )
        data | "PrintData" >> beam.Map(lambda element : logging.log(logging.INFO, "Actual log:"+str(element)))
        # data | "WriteToSpanner" >> beam.io.WriteToSpanner(
        #     instance_id=pipeline_options.spanner_instance_id.get(),
        #     database_id=pipeline_options.spanner_database_id.get(),
        #     table_id=pipeline_options.spanner_table_id.get(),
        # )
for this code I am using below command to run it after creating template:
 gcloud dataflow jobs run wireline_churn_bq_spanner1018 \
--gcs-location gs://vznet-test/wireline_churn_test/code/wireline_churn_bq_spanner \
--region us-east4 \
--num-workers 2 \
--max-workers 2 \
--worker-machine-type n2-standard-2 \
--disable-public-ips \
--network=shared-np-east \
--dataflow-kms-key=projects/vz-it-np-d0sv-vsadkms-0/locations/us-east4/keyRings/vz-it-np-kr-aid/cryptoKeys/vz-it-np-kms-gudv \
--subnetwork=https://www.googleapis.com/compute/v1/projects/vz-it-np-exhv-sharedvpc-228116/regions/us-east4/subnetworks/shared-np-east-green-subnet-2 \
--service-account-email sa-dev-gudv-app-vzntdo-0@vz-it-np-gudv-dev-vzntdo-0.iam.gserviceaccount.com \
--parameters \
input_table=vz-it-np-gudv-dev-vzntdo-0.test_dataset.dim_inventory_customer_profiles_norm_v0,\
spanner_instance_id=sample_instance_id,\
spanner_database_id=sample_database_id,\
spanner_table_id=sample_table
ERROR: (gcloud.dataflow.jobs.run) INVALID_ARGUMENT: (e400ba4cc2dcbd34): The workflow could not be created. Causes: (e400ba4cc2dcbc9d): Found unexpected parameters: ['input_table' (perhaps you meant 'update'), 'spanner_database_id' (perhaps you meant 'save_main_session'), 'spanner_instance_id' (perhaps you meant 'spark_master_url'), 'spanner_table_id' (perhaps you meant 'parallelism')]
