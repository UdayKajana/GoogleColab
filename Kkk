import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from fastavro import reader
import io as inputoutput
import csv
import typing
import logging
logging.getLogger().setLevel(logging.INFO)
project_id = "vz-it-np-gudv-dev-vzntdo-0"
subscription_id = "wireline_churn_test_topic-sub"
subscription_path = f"projects/{project_id}/subscriptions/{subscription_id}"
gcs_bucket = "vznet-test"
gcs_output_path = "wireline_churn_test/tgt/customer_profile"

schema = [
    "ont_activation_date",
    "data_circuit_id",
    "circuit_id",
    "video_circuit_id",
    "service_type",
    "address_id",
    "vision_account_id",
    "vision_customer_id",
    "address_type",
    "line_of_business"
]



import io
import json
import traceback
import pandas as pd
from google.cloud import pubsub_v1
import avro.schema
from avro.io import DatumReader, BinaryDecoder
from io import BytesIO
import fastavro

def extract_avro_schema_from_message(message_data):
    """
    Attempt to extract the Avro schema from the message
    """
    try:
        # Use fastavro to read the schema
        bytes_reader = BytesIO(message_data)
        reader = fastavro.reader(bytes_reader)
        
        # Extract the writer's schema
        writer_schema = reader.writer_schema
        print("Extracted Writer's Schema:")
        print(json.dumps(writer_schema, indent=2))
        
        return writer_schema
    except Exception as e:
        print(f"Error extracting schema: {e}")
        return None

def process_avro_message(message):
    """
    Robust Avro message processing with dynamic schema handling
    """
    try:
        # Print basic message info
        print(f"--- Message Debug ---")
        print(f"Message type: {type(message.data)}")
        print(f"Message length: {len(message.data)}")
        
        # Extract the writer's schema
        writer_schema = extract_avro_schema_from_message(message.data)
        if not writer_schema:
            print("Failed to extract schema")
            message.nack()
            return None
        
        # Reset the bytes reader
        bytes_reader = BytesIO(message.data)
        
        # Use fastavro to read records
        rows = []
        try:
            for record in fastavro.reader(bytes_reader, writer_schema):
                # Process each record
                processed_record = {}
                for key, value in record.items():
                    # Handle various value types
                    if value is None:
                        processed_record[key] = None
                    elif isinstance(value, (dict, list)):
                        processed_record[key] = str(value)
                    else:
                        processed_record[key] = value
                
                rows.append(processed_record)
            
            # Save processed records
            if rows:
                df = pd.DataFrame(rows)
                csv_filename = f'pubsub_data_{pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")}.csv'
                df.to_csv(csv_filename, index=False)
                print(f"Saved {len(df)} records to {csv_filename}")
                message.ack()
                return df
            else:
                print("No records could be processed from the message")
                message.ack()
                return None
        
        except Exception as read_error:
            print(f"Error reading Avro records: {read_error}")
            print(traceback.format_exc())
            message.nack()
            return None
    
    except Exception as overall_error:
        print(f"Overall message processing error: {overall_error}")
        print(traceback.format_exc())
        message.nack()
        return None
def run_pipeline():
    import apache_beam as beam
    with beam.Pipeline(options=pipeline_options) as pipeline:
        messages = (
            pipeline
            | "ReadFromPubSub" >> beam.io.ReadFromPubSub(subscription=subscription_path)
        )
        parsed_data = (
            messages
            | "ParseAvroMessages" >> beam.FlatMap(parse_avro_message)
        )
        print_data = (parsed_data|"Print" >> beam.Map(Print_element))
        parsed_data | "WriteToGCS" >> beam.Map(lambda x: beam.io.WriteToText(
            f"gs://vznet-test/wireline_churn_test/tgt/",
            file_name_suffix=".csv",
            num_shards=1 
        ))
        # csv_data = (
        #     parsed_data
        #     | "FormatToCSV" >> beam.Map(create_unique_filename)
        # )
        # print_data = (csv_data|"Print" >> beam.Map(Print_element))
        # # csv_data | "WriteToGCS" >> beam.Map(lambda x: beam.io.WriteToText(
        # #     f"gs://vznet-test/wireline_churn_test/tgt/",
        # #     file_name_suffix=".csv",
        # #     num_shards=1 
        # # ))

def Print_element(element):
    logging.info("element="+str(element))
    return element
pipeline_options = PipelineOptions(
    streaming=True,
    project=project_id,
    temp_location=f"gs://{gcs_bucket}/wireline_churn_test/tmp",
    region="us-central1"
)

if __name__ == '__main__':
    run_pipeline()
