import apache_beam as beam
import logging
import json
import traceback
import pandas as pd
import fastavro
from io import BytesIO
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.transforms.combiners import CountCombineFn

# Configure logging
logging.getLogger().setLevel(logging.INFO)

# Configuration parameters
PROJECT_ID = "vz-it-np-gudv-dev-vzntdo-0"
SUBSCRIPTION_ID = "wireline_churn_test_topic-sub"
GCS_BUCKET = "vznet-test"
GCS_OUTPUT_PATH = "wireline_churn_test/tgt/customer_profile"

def extract_avro_message(message):
    """
    Robust method to extract and parse Avro message
    
    Args:
        message: Pub/Sub message containing Avro-encoded data
    
    Returns:
        List of parsed records or None if parsing fails
    """
    try:
        # Read Avro data from message bytes
        bytes_reader = BytesIO(message)
        
        # Use fastavro to read records with auto schema detection
        records = []
        try:
            for record in fastavro.reader(bytes_reader):
                # Standardize record processing
                processed_record = {}
                for key, value in record.items():
                    # Handle various value types
                    if value is None:
                        processed_record[key] = None
                    elif isinstance(value, (dict, list)):
                        processed_record[key] = json.dumps(value)
                    else:
                        processed_record[key] = value
                
                records.append(processed_record)
            
            return records
        
        except Exception as read_error:
            logging.error(f"Error reading Avro records: {read_error}")
            logging.error(traceback.format_exc())
            return None
    
    except Exception as overall_error:
        logging.error(f"Overall message processing error: {overall_error}")
        logging.error(traceback.format_exc())
        return None

def run_pubsub_avro_pipeline():
    """
    Main pipeline to:
    1. Read from Pub/Sub 
    2. Process Avro messages
    3. Write processed data to GCS as CSV
    """
    # Subscription path
    subscription_path = f"projects/{PROJECT_ID}/subscriptions/{SUBSCRIPTION_ID}"
    
    # Pipeline options
    pipeline_options = PipelineOptions([
        '--project', PROJECT_ID,
        '--job_name', 'wireline-churn-avro-pipeline',
        '--streaming',
        '--region', 'us-central1',
        f'--temp_location=gs://{GCS_BUCKET}/wireline_churn_test/tmp',
        '--runner', 'DataflowRunner'
    ])
    
    with beam.Pipeline(options=pipeline_options) as pipeline:
        # Read messages from Pub/Sub
        messages = (
            pipeline 
            | 'ReadFromPubSub' >> beam.io.ReadFromPubSub(
                subscription=subscription_path, 
                with_attributes=True
            )
        )
        
        # Extract and parse Avro records
        parsed_records = (
            messages
            | 'DecodeAvroMessages' >> beam.Map(lambda msg: extract_avro_message(msg.data))
            | 'FlattenRecords' >> beam.FlatMap(lambda x: x or [])  # Filter out None results
        )
        
        # Convert to DataFrame and write to GCS
        csv_files = (
            parsed_records
            | 'CreateDataFrame' >> beam.Map(lambda records: pd.DataFrame(records))
            | 'WriteToGCS' >> beam.Map(
                lambda df: df.to_csv(
                    f"gs://{GCS_BUCKET}/{GCS_OUTPUT_PATH}/customer_profile_{beam.DoFn.current_element_info().timestamp}.csv", 
                    index=False
                )
            )
        )
        
        # Optional: Log processing statistics
        _ = (
            parsed_records
            | 'CountRecords' >> beam.CombineGlobally(CountCombineFn())
            | 'LogRecordCount' >> beam.Map(lambda count: logging.info(f'Total records processed: {count}'))
        )

if __name__ == '__main__':
    run_pubsub_avro_pipeline()
