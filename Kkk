import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions
from fastavro import reader
import io
import csv
import uuid
from datetime import datetime
from google.cloud import storage

class AvroToCsvPipeline:
    def __init__(self):
        # Configuration
        self.PROJECT_ID = "vz-it-np-gudv-dev-vzntdo-0"
        self.SUBSCRIPTION_ID = "wireline_churn_test_topic-sub"
        self.SUBSCRIPTION_PATH = f"projects/{self.PROJECT_ID}/subscriptions/{self.SUBSCRIPTION_ID}"
        self.GCS_BUCKET = "vznet-test"
        self.GCS_OUTPUT_PATH = "wireline_churn_test/tgt/customer_profile"

        # Avro schema (ensure this matches your Avro file structure)
        self.SCHEMA = [
            "ont_activation_date",
            "data_circuit_id",
            "circuit_id",
            "video_circuit_id",
            "service_type",
            "address_id",
            "vision_account_id",
            "vision_customer_id",
            "address_type",
            "line_of_business"
        ]

        # Configure pipeline options for streaming
        self.pipeline_options = PipelineOptions([
            '--streaming',
            f'--project={self.PROJECT_ID}',
            f'--temp_location=gs://{self.GCS_BUCKET}/wireline_churn_test/tmp',
            '--region=us-central1'
        ])
        
        # Ensure streaming is enabled
        self.pipeline_options.view_as(StandardOptions).streaming = True

    def process_avro_message(self, message):
        """
        Process a single Avro message and convert to CSV.
        
        Args:
            message: Pub/Sub message containing a complete Avro file
        
        Returns:
            Filename and CSV content
        """
        try:
            # Extract Avro bytes from the message
            avro_bytes = message.data
            avro_file = io.BytesIO(avro_bytes)
            avro_reader = reader(avro_file)
            
            # Generate unique filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            unique_id = str(uuid.uuid4())[:8]
            filename = f"customer_profile_{timestamp}_{unique_id}.csv"
            
            # Create CSV content
            output = io.StringIO()
            writer = csv.DictWriter(output, fieldnames=self.SCHEMA)
            
            # Write header and records
            writer.writeheader()
            for record in avro_reader:
                # Ensure only records with the defined schema are written
                filtered_record = {
                    field: record.get(field, '') 
                    for field in self.SCHEMA
                }
                writer.writerow(filtered_record)
            
            # Return filename and CSV content
            return (filename, output.getvalue())
        
        except Exception as e:
            print(f"Error processing Avro message: {e}")
            return None

    def write_to_gcs(self, filename_and_content):
        """
        Write content to GCS with the given filename.
        
        Args:
            filename_and_content (tuple): (filename, CSV content)
        """
        if filename_and_content is None:
            return
        
        filename, content = filename_and_content
        
        storage_client = storage.Client()
        bucket = storage_client.bucket(self.GCS_BUCKET)
        blob = bucket.blob(f"{self.GCS_OUTPUT_PATH}/{filename}")
        
        blob.upload_from_string(content)
        print(f"Uploaded {filename} to gs://{self.GCS_BUCKET}/{self.GCS_OUTPUT_PATH}")

    def run(self):
        """
        Execute the Avro to CSV streaming pipeline.
        """
        with beam.Pipeline(options=self.pipeline_options) as pipeline:
            # Read from Pub/Sub
            messages = (
                pipeline
                | "ReadFromPubSub" >> beam.io.ReadFromPubSub(subscription=self.SUBSCRIPTION_PATH)
            )
            
            # Process each Avro message into a separate CSV
            csv_files = (
                messages
                | "ProcessAvroToCsv" >> beam.Map(self.process_avro_message)
            )
            
            # Write CSV files to GCS
            csv_files | "WriteToGCS" >> beam.Map(self.write_to_gcs)

def main():
    """
    Main entry point for the Avro to CSV streaming pipeline.
    """
    pipeline = AvroToCsvPipeline()
    pipeline.run()

if __name__ == '__main__':
    main()
