# pipeline.py
import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions, SetupOptions
import argparse

class CustomPipelineOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
        # Required parameters
        parser.add_argument(
            '--input_bq_table',
            required=True,
            help='Input BigQuery table in format: dataset.table'
        )
        parser.add_argument(
            '--output_gcs_path',
            required=True,
            help='Output GCS path'
        )
        # Optional parameters with defaults
        parser.add_argument(
            '--region',
            default='us-east4',
            help='GCP region for the pipeline'
        )
        parser.add_argument(
            '--staging_location',
            help='GCS path for staging'
        )
        parser.add_argument(
            '--temp_location',
            help='GCS path for temporary files'
        )

def run(argv=None):
    parser = argparse.ArgumentParser()
    known_args, pipeline_args = parser.parse_known_args(argv)
    
    # Create pipeline options
    pipeline_options = CustomPipelineOptions(pipeline_args)
    
    # Set specific pipeline options
    pipeline_options.view_as(StandardOptions).runner = 'DataflowRunner'
    
    # If staging_location and temp_location are provided, set them
    if pipeline_options.staging_location:
        pipeline_options.view_as(SetupOptions).staging_location = pipeline_options.staging_location
    if pipeline_options.temp_location:
        pipeline_options.view_as(SetupOptions).temp_location = pipeline_options.temp_location
    
    # Create the pipeline
    with beam.Pipeline(options=pipeline_options) as pipeline:
        (pipeline 
         | 'Read from BigQuery' >> beam.io.ReadFromBigQuery(
             table=known_args.input_bq_table,
             project=pipeline_options.project)
         | 'Write to GCS' >> beam.io.WriteToText(
             known_args.output_gcs_path)
        )

if __name__ == '__main__':
    run()

# metadata.json
{
    "name": "Flex Template Example",
    "description": "A simple BigQuery to GCS pipeline using Flex Template",
    "parameters": [
        {
            "name": "input_bq_table",
            "label": "Input BigQuery Table",
            "helpText": "Input BigQuery table in format: dataset.table"
        },
        {
            "name": "output_gcs_path",
            "label": "Output GCS Path",
            "helpText": "Output GCS path where the results will be written"
        },
        {
            "name": "region",
            "label": "Region",
            "helpText": "GCP region for the pipeline",
            "isOptional": true,
            "defaultValue": "us-east4"
        },
        {
            "name": "staging_location",
            "label": "Staging Location",
            "helpText": "GCS path for staging",
            "isOptional": true
        },
        {
            "name": "temp_location",
            "label": "Temp Location",
            "helpText": "GCS path for temporary files",
            "isOptional": true
        }
    ]
}

# requirements.txt
apache-beam[gcp]==2.50.0
