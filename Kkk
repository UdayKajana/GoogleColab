import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
import io
import csv
import logging
import uuid
import argparse

logging.getLogger().setLevel(logging.INFO)

class BQToGCSOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
        parser.add_value_provider_argument(
            '--input_subscription',
            type=str,
            help='pubsub subscription path'
        )
        parser.add_value_provider_argument(
            '--output_path',
            type=str,
            help='Output GCS path (e.g., gs://bucket/path/prefix)'
        )

def process_avro_to_csv(message_data):
    logging.log(logging.INFO, str(message_data))
    from io import BytesIO
    import io
    import fastavro
    import csv
    import uuid
    try:
        logging.log(logging.INFO, str(BytesIO))
        # Read Avro data
        bytes_reader = BytesIO(message_data)
        avro_reader = fastavro.reader(bytes_reader)

        # Extract records
        records = [record for record in avro_reader]
        if not records:
            logging.warning("No records found in the Avro message.")
            return None

        # Convert records to CSV format
        fieldnames = list(records[0].keys())
        csv_buffer = io.StringIO()
        writer = csv.DictWriter(csv_buffer, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(records)

        # Generate a unique filename
        unique_id = uuid.uuid4()
        filename = f"message_{unique_id}.csv"

        logging.info(f"Processed message into file: {filename}")
        return filename, csv_buffer.getvalue()
    except Exception as e:
        logging.error(f"Error processing Avro message: {e}")
        return None

class WriteToGCS(beam.DoFn):
    def __init__(self, output_path):
        self.output_path = output_path
        
    def process(self, element):
        from apache_beam.io.gcp.gcsio import GcsIO
        try:
            filename, csv_data = element
            if not csv_data:
                logging.warning(f"No CSV data to write for {filename}. Skipping.")
                return

            # Get the output path at runtime
            output_path = self.output_path.get()
            
            # Write CSV data to GCS
            gcs_io = GcsIO()
            with gcs_io.open(output_path, 'w') as gcs_file:
                gcs_file.write(csv_data.encode("utf-8"))

            logging.info(f"Successfully wrote {filename} to GCS: {output_path}")
        except Exception as e:
            logging.error(f"Error writing CSV to GCS: {e}")

def run_pipeline(argv=None):
    parser = argparse.ArgumentParser()
    known_args, pipeline_args = parser.parse_known_args(argv)
    pipeline_options = BQToGCSOptions(pipeline_args)

    with beam.Pipeline(options=pipeline_options) as pipeline:
        messages = (pipeline 
                   | "ReadFromPubSub" >> beam.io.ReadFromPubSub(
                       subscription=pipeline_options.input_subscription)
                   )
        
        csv_files = (messages
                    | "ProcessAvroToCSV" >> beam.Map(process_avro_to_csv)
                    | "FilterNone" >> beam.Filter(lambda x: x is not None)
                    )
        
        _ = (csv_files
             | "WriteCSVToGCS" >> beam.ParDo(WriteToGCS(pipeline_options.output_path)))

if __name__ == '__main__':
    run_pipeline()
