import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions, SetupOptions
import logging
import argparse

class PrintParameters(beam.DoFn):
    def process(self, element, input_bq_table, output_gcs_path):
        logging.info(f"Received element: {element}")
        logging.info(f"Input BigQuery table: {input_bq_table}")
        logging.info(f"Output GCS path: {output_gcs_path}")
        yield element

def run(argv=None):
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--input_bq_table',
        required=True,
        help='Input BigQuery table, in format "project:dataset.table".'
    )
    parser.add_argument(
        '--output_gcs_path',
        required=True,
        help='Output GCS path for storing results.'
    )
    known_args, pipeline_args = parser.parse_known_args(argv)

    # Define PipelineOptions
    pipeline_options = PipelineOptions(pipeline_args)
    pipeline_options.view_as(SetupOptions).save_main_session = True

    # Log the input parameters
    logging.info(f"Running pipeline with parameters: {known_args}")
    
    # Create the pipeline
    with beam.Pipeline(options=pipeline_options) as p:
        # Read from BigQuery table
        rows = p | 'ReadFromBigQuery' >> beam.io.ReadFromBigQuery(table=known_args.input_bq_table)

        # Process the data: Print the parameters and the data
        processed_data = rows | 'PrintParameters' >> beam.ParDo(PrintParameters(), 
                                                                input_bq_table=known_args.input_bq_table, 
                                                                output_gcs_path=known_args.output_gcs_path)

        # Optionally, write to a GCS location or another sink
        processed_data | 'WriteToGCS' >> beam.io.WriteToText(known_args.output_gcs_path)

if __name__ == '__main__':
    run()




from setuptools import setup, find_packages

setup(
    name='dataflow_flex_template',
    version='0.1',
    packages=find_packages(),
    install_requires=[
        'apache-beam[gcp]',
        'google-cloud-bigquery',
        'google-cloud-storage',
    ],
)




{
  "image": "gcr.io/dataflow-templates/v1/templates/beam-python-template",
  "sdk_info": {
    "language": "PYTHON"
  },
  "parameters": [
    {
      "name": "input_bq_table",
      "label": "Input BigQuery Table",
      "helpText": "BigQuery table to read from, in the format 'project:dataset.table'."
    },
    {
      "name": "output_gcs_path",
      "label": "Output GCS Path",
      "helpText": "GCS path for storing the output files (e.g., gs://your-bucket/output/)."
    }
  ],
  "containerSpec": {
    "image": "gcr.io/<your-project-id>/dataflow-flextemplate"
  }
}
