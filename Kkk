import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.transforms.window import FixedWindows
from apache_beam.transforms.trigger import AfterProcessingTime, Repeatedly, AccumulationMode
from fastavro import reader
import io
import csv

# Define your Pub/Sub subscription and GCS bucket details
project_id = "vz-it-np-gudv-dev-vzntdo-0"
subscription_id = "wireline_churn_test_topic-sub"
subscription_path = f"projects/{project_id}/subscriptions/{subscription_id}"
gcs_bucket = "vznet-test"
gcs_output_path = "wireline_churn_test/tgt/customer_profile"

# Avro schema
schema = [
    "ont_activation_date",
    "data_circuit_id",
    "circuit_id",
    "video_circuit_id",
    "service_type",
    "address_id",
    "vision_account_id",
    "vision_customer_id",
    "address_type",
    "line_of_business"
]

# Function to parse Avro messages
def parse_avro_message(message):
    avro_bytes = message.data
    avro_file = io.BytesIO(avro_bytes)
    avro_reader = reader(avro_file)
    for record in avro_reader:
        # Use a key (e.g., address_id) for grouping
        yield (record["address_id"], record)

# Function to format grouped records into CSV
def format_grouped_records(key_and_records):
    key, records = key_and_records
    output = io.StringIO()
    writer = csv.DictWriter(output, fieldnames=schema)
    writer.writeheader()
    for record in records:
        writer.writerow(record)
    return output.getvalue()

# Define the Beam pipeline options
pipeline_options = PipelineOptions(
    streaming=True,
    project=project_id,
    temp_location=f"gs://{gcs_bucket}/temp",
    region="us-central1"
)

# Define the Beam pipeline
with beam.Pipeline(options=pipeline_options) as pipeline:
    # Step 1: Read messages from Pub/Sub
    messages = (
        pipeline
        | "ReadFromPubSub" >> beam.io.ReadFromPubSub(subscription=subscription_path)
    )

    # Step 2: Apply Fixed Windowing
    windowed_data = (
        messages
        | "WindowIntoFixed" >> beam.WindowInto(
            FixedWindows(60),  # 1-minute windows
            trigger=Repeatedly(AfterProcessingTime(30)),  # Emit every 30 seconds
            accumulation_mode=AccumulationMode.DISCARDING
        )
    )

    # Step 3: Parse Avro messages and assign keys
    parsed_data = (
        windowed_data
        | "ParseAvroMessages" >> beam.FlatMap(parse_avro_message)  # (key, record)
    )

    # Step 4: Group by Key within the window
    grouped_data = (
        parsed_data
        | "GroupByKey" >> beam.GroupByKey()
    )

    # Step 5: Format grouped records as CSV
    csv_data = (
        grouped_data
        | "FormatToCSV" >> beam.Map(format_grouped_records)
    )

    # Step 6: Write the CSV data to GCS
    csv_data | "WriteToGCS" >> beam.io.WriteToText(
        f"gs://{gcs_bucket}/{gcs_output_path}",
        file_name_suffix=".csv",
        shard_name_template=""  # Avoid file sharding
    )
