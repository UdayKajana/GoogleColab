import apache_beam as beam
from apache_beam.io.avroio import ReadFromAvro
from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions, GoogleCloudOptions, SetupOptions
import csv
import io
import uuid
from google.cloud import storage
import argparse

class AvroToCsvPipeline:
    def __init__(self):
        """
        Initialize the Avro to CSV conversion pipeline with configurable options.
        """
        # Parse command-line arguments
        self.parse_arguments()
        
        # Configure pipeline options
        self.pipeline_options = self.configure_pipeline_options()
    
    def parse_arguments(self):
        """
        Define and parse command-line arguments for the pipeline.
        """
        parser = argparse.ArgumentParser(description='Avro to CSV Dataflow Pipeline')
        
        # Input and output configurations
        parser.add_argument('--input_avro', 
                            required=True, 
                            help='Full path to the input Avro file in GCS (gs://bucket/path/file.avro)')
        parser.add_argument('--output_bucket', 
                            required=True, 
                            help='GCS bucket name for output CSVs (bucket-name)')
        parser.add_argument('--output_path', 
                            default='avro_to_csv_output/', 
                            help='Output path within the bucket (default: avro_to_csv_output/)')
        
        # GCP Project and Dataflow configurations
        parser.add_argument('--project', 
                            required=True, 
                            help='Google Cloud Project ID')
        parser.add_argument('--region', 
                            default='us-central1', 
                            help='GCP region for Dataflow job (default: us-central1)')
        parser.add_argument('--staging_location', 
                            help='GCS staging location (gs://bucket/staging)')
        parser.add_argument('--temp_location', 
                            help='GCS temporary location (gs://bucket/temp)')
        
        # Runner configuration
        parser.add_argument('--runner', 
                            default='DataflowRunner', 
                            choices=['DirectRunner', 'DataflowRunner'], 
                            help='Pipeline runner (default: DataflowRunner)')
        
        # Parse the arguments
        self.args = parser.parse_args()
    
    def configure_pipeline_options(self):
        """
        Configure comprehensive pipeline options for Dataflow.
        
        Returns:
            PipelineOptions: Configured pipeline options
        """
        # Create pipeline options
        options = PipelineOptions()
        
        # Set Google Cloud specific configurations
        google_cloud_options = options.view_as(GoogleCloudOptions)
        google_cloud_options.project = self.args.project
        google_cloud_options.region = self.args.region
        
        # Set staging and temp locations (if provided)
        if self.args.staging_location:
            google_cloud_options.staging_location = self.args.staging_location
        else:
            # Auto-generate staging location if not provided
            google_cloud_options.staging_location = f'gs://{self.args.output_bucket}/staging'
        
        if self.args.temp_location:
            google_cloud_options.temp_location = self.args.temp_location
        else:
            # Auto-generate temp location if not provided
            google_cloud_options.temp_location = f'gs://{self.args.output_bucket}/temp'
        
        # Set runner
        options.view_as(StandardOptions).runner = self.args.runner
        
        # Disable manual job name generation if using DataflowRunner
        if self.args.runner == 'DataflowRunner':
            google_cloud_options.job_name = f'avro-to-csv-{uuid.uuid4().hex[:10]}'
        
        # Configure setup options
        setup_options = options.view_as(SetupOptions)
        setup_options.save_main_session = True
        
        return options
    
    def convert_to_csv(self, element):
        """
        Convert a single Avro record to a CSV string.
        
        Args:
            element (dict): A single Avro record
        
        Returns:
            tuple: (output_filename, csv_content)
        """
        # Generate a unique filename for each record
        unique_id = str(uuid.uuid4())
        output_filename = f'record_{unique_id}.csv'
        
        # Convert the record to a CSV string
        output = io.StringIO()
        writer = csv.writer(output)
        
        # Write headers
        writer.writerow(element.keys())
        
        # Write data
        writer.writerow(element.values())
        
        return (output_filename, output.getvalue())
    
    def upload_to_gcs(self, record):
        """
        Upload a single CSV record to GCS.
        
        Args:
            record (tuple): (filename, file_content)
        """
        filename, content = record
        storage_client = storage.Client()
        bucket = storage_client.bucket(self.args.output_bucket)
        blob = bucket.blob(f"{self.args.output_path}{filename}")
        
        blob.upload_from_string(content)
        print(f"File {filename} uploaded to {self.args.output_bucket}")
    
    def run(self):
        """
        Execute the Avro to CSV conversion pipeline.
        """
        with beam.Pipeline(options=self.pipeline_options) as p:
            # Read Avro file
            records = p | 'Read Avro' >> ReadFromAvro(self.args.input_avro)
            
            # Convert records to CSV
            csv_records = records | 'Convert to CSV' >> beam.Map(self.convert_to_csv)
            
            # Upload each CSV to GCS
            csv_records | 'Upload to GCS' >> beam.Map(self.upload_to_gcs)

def main():
    """
    Main entry point for the Avro to CSV Dataflow pipeline.
    """
    pipeline = AvroToCsvPipeline()
    pipeline.run()

if __name__ == '__main__':
    main()

# Example usage command (for local reference):
# python avro_to_csv_dataflow.py \
#     --input_avro=gs://your-input-bucket/input.avro \
#     --output_bucket=your-output-bucket \
#     --project=your-gcp-project
