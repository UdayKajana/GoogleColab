import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
project_id = "vz-it-np-gudv-dev-vzntdo-0"
dataset_id = 'test_dataset'
table_id = 'dim_inventory_customer_profiles_norm_v0'

# Define your GCS bucket and folder details
gcs_bucket = "vznet-test"
folder_name = 'wireline_churn_test'

# Define your BigQuery query
query = """
    SELECT *
    FROM `vz-it-np-gudv-dev-vzntdo-0.test_dataset.dim_inventory_customer_profiles_norm_v0`
"""

# Define your pipeline options
pipeline_options = PipelineOptions(
    runner='DataflowRunner',
    project=project_id,
    region='us-central1',
    temp_location="gs://vznet-test/wireline_churn_test/tmp",
    staging_location="gs://vznet-test/wireline_churn_test/stg",
)

# Create your pipeline
with beam.Pipeline(options=pipeline_options) as pipeline:
    # Read data from BigQuery
    data = pipeline | 'ReadFromBigQuery' >> beam.io.ReadFromBigQuery(
        query=query,
        use_standard_sql=True,
    )

    # Convert data to CSV format
    csv_data = data | 'ConvertToCSV' >> beam.Map(lambda row: ','.join(str(x) for x in row.values()))

    # Write data to GCS
    csv_data | 'WriteToGCS' >> beam.io.WriteToText(
        f'gs://vznet-test/wireline_churn_test/output.csv',
        file_name_suffix='.csv',
        header='col1,col2,...',  
    )
