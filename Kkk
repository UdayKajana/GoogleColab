import apache_beam as beam
import logging
import json
import traceback
import csv
import fastavro
from io import BytesIO, StringIO
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.transforms.combiners import CountCombineFn
from apache_beam.io.textio import WriteToText

# Configure logging
logging.getLogger().setLevel(logging.INFO)

# Configuration parameters
PROJECT_ID = "vz-it-np-gudv-dev-vzntdo-0"
SUBSCRIPTION_ID = "wireline_churn_test_topic-sub"
GCS_BUCKET = "vznet-test"
GCS_OUTPUT_PATH = "wireline_churn_test/tgt/customer_profile"

def extract_avro_message(message):
    """
    Robust method to extract and parse Avro message
    
    Args:
        message: Pub/Sub message containing Avro-encoded data
    
    Returns:
        List of parsed records or None if parsing fails
    """
    try:
        # Read Avro data from message bytes
        bytes_reader = BytesIO(message)
        
        # Use fastavro to read records with auto schema detection
        records = []
        try:
            for record in fastavro.reader(bytes_reader):
                # Standardize record processing
                processed_record = {}
                for key, value in record.items():
                    # Handle various value types
                    if value is None:
                        processed_record[key] = ''
                    elif isinstance(value, (dict, list)):
                        processed_record[key] = json.dumps(value)
                    else:
                        processed_record[key] = str(value)
                
                records.append(processed_record)
            
            return records
        
        except Exception as read_error:
            logging.error(f"Error reading Avro records: {read_error}")
            logging.error(traceback.format_exc())
            return None
    
    except Exception as overall_error:
        logging.error(f"Overall message processing error: {overall_error}")
        logging.error(traceback.format_exc())
        return None

def convert_to_csv_line(record):
    """
    Convert a record dictionary to a CSV line
    
    Args:
        record (dict): A single record from Avro message
    
    Returns:
        str: CSV formatted line
    """
    # If this is the first record, use its keys as headers
    if not hasattr(convert_to_csv_line, 'headers'):
        convert_to_csv_line.headers = list(record.keys())
        yield ','.join(convert_to_csv_line.headers)
    
    # Convert record to CSV line
    csv_line = ','.join([
        f'"{str(record.get(header, ""))}"' for header in convert_to_csv_line.headers
    ])
    yield csv_line

def run_pubsub_avro_pipeline():
    """
    Main pipeline to:
    1. Read from Pub/Sub 
    2. Process Avro messages
    3. Write processed data to GCS as CSV
    """
    # Subscription path
    subscription_path = f"projects/{PROJECT_ID}/subscriptions/{SUBSCRIPTION_ID}"
    
    # Unique output path for each pipeline run
    output_path = f"gs://{GCS_BUCKET}/{GCS_OUTPUT_PATH}/customer_profile"
    
    # Pipeline options
    pipeline_options = PipelineOptions([
        '--project', PROJECT_ID,
        '--job_name', 'wireline-churn-avro-pipeline',
        '--streaming',
        '--region', 'us-central1',
        f'--temp_location=gs://{GCS_BUCKET}/wireline_churn_test/tmp',
        '--runner', 'DataflowRunner'
    ])
    
    with beam.Pipeline(options=pipeline_options) as pipeline:
        # Read messages from Pub/Sub
        messages = (
            pipeline 
            | 'ReadFromPubSub' >> beam.io.ReadFromPubSub(
                subscription=subscription_path, 
                with_attributes=True
            )
        )
        
        # Extract and parse Avro records
        parsed_records = (
            messages
            | 'DecodeAvroMessages' >> beam.Map(lambda msg: extract_avro_message(msg.data))
            | 'FlattenRecords' >> beam.FlatMap(lambda x: x or [])  # Filter out None results
        )
        
        # Convert records to CSV lines
        csv_lines = (
            parsed_records
            | 'ConvertToCSVLines' >> beam.FlatMap(convert_to_csv_line)
        )
        
        # Write to GCS
        _ = (
            csv_lines
            | 'WriteToGCS' >> WriteToText(
                output_path, 
                file_name_suffix='.csv', 
                num_shards=1  # You can adjust sharding as needed
            )
        )
        
        # Optional: Log processing statistics
        _ = (
            parsed_records
            | 'CountRecords' >> beam.CombineGlobally(CountCombineFn())
            | 'LogRecordCount' >> beam.Map(lambda count: logging.info(f'Total records processed: {count}'))
        )

if __name__ == '__main__':
    run_pubsub_avro_pipeline()
