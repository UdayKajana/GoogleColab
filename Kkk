import apache_beam as beam
from apache_beam.io.gcp.spanner import WriteToSpanner
from apache_beam.options.pipeline_options import PipelineOptions
from io import BytesIO
import fastavro
import logging

logging.getLogger().setLevel(logging.INFO)

# Spanner details
project_id = "your-project-id"
instance_id = "my-instance"
database_id = "my-database"
table_name = "my_table"

# Subscription details
subscription_id = "your-subscription-id"
subscription_path = f"projects/{project_id}/subscriptions/{subscription_id}"

# Define table schema
table_columns = [
    "column1",
    "column2",
    "column3"
]


def parse_avro_message(message):
    """
    Parse Avro message and map it to Spanner table structure.
    """
    from io import BytesIO
    try:
        # Read Avro data
        bytes_reader = BytesIO(message)
        avro_reader = fastavro.reader(bytes_reader)
        rows = []

        # Extract records and map to Spanner schema
        for record in avro_reader:
            spanner_row = {column: record.get(column, None) for column in table_columns}
            rows.append(spanner_row)
        
        return rows
    except Exception as e:
        logging.error(f"Error parsing Avro message: {e}")
        return []


def run_pipeline():
    # Pipeline options
    pipeline_options = PipelineOptions(
        streaming=True,
        project=project_id,
        temp_location=f"gs://your-bucket/temp",
        region="us-central1"
    )

    with beam.Pipeline(options=pipeline_options) as pipeline:
        # Read messages from Pub/Sub
        messages = (pipeline 
                    | "ReadFromPubSub" >> beam.io.ReadFromPubSub(subscription=subscription_path)
                   )

        # Parse Avro messages
        parsed_rows = (messages
                       | "ParseAvroMessages" >> beam.FlatMap(parse_avro_message)
                      )

        # Write to Spanner
        _ = (parsed_rows
             | "WriteToSpanner" >> WriteToSpanner(
                 project_id=project_id,
                 instance_id=instance_id,
                 database_id=database_id,
                 table=table_name
             ))

if __name__ == '__main__':
    run_pipeline()
