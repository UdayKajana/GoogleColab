import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions
import logging
import argparse

logging.getLogger().setLevel(logging.INFO)

def run_pipeline():
    parser = argparse.ArgumentParser()
    # Add all arguments
    parser.add_argument('--project', required=True)
    parser.add_argument('--input_bq_table', required=True)
    parser.add_argument('--output_gcs_path', required=True)
    parser.add_argument('--region', required=True)
    parser.add_argument('--temp_location', required=True)
    parser.add_argument('--staging_location', required=True)
    parser.add_argument('--runner', required=True)

    # Parse known args and create pipeline options
    known_args, pipeline_args = parser.parse_known_args()
    pipeline_options = PipelineOptions(pipeline_args)

    # Print the submitted inputs
    logging.info("Job Inputs:")
    logging.info(f"Project ID: {known_args.project}")
    logging.info(f"Input BigQuery Table: {known_args.input_bq_table}")
    logging.info(f"Output GCS Path: {known_args.output_gcs_path}")
    logging.info(f"Region: {known_args.region}")
    logging.info(f"Temporary Location: {known_args.temp_location}")
    logging.info(f"Staging Location: {known_args.staging_location}")

    # Minimal pipeline definition
    with beam.Pipeline(options=pipeline_options) as pipeline:
        _ = (pipeline 
             | "CreateDummyPCollection" >> beam.Create([])  # No actual processing
            )

if __name__ == '__main__':
    run_pipeline()
