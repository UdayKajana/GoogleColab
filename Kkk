import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
import io
import csv
import logging
import uuid
import argparse
logging.getLogger().setLevel(logging.INFO)
global pipeline_options
class BQToGCSOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
        parser.add_value_provider_argument(
            '--input_subscription',
            type=str,
            help='pubsub subscription path'
        )
        parser.add_value_provider_argument(
            '--output_path',
            type=str,
            help='Output GCS path (e.g., gs://bucket/path/prefix)'
        )

def process_avro_to_csv(message_data):
    logging.log(logging.INFO, str(message_data))
    from io import BytesIO
    import io
    import fastavro
    import csv
    import uuid
    try:
        logging.log(logging.INFO, str(BytesIO))
        # Read Avro data
        bytes_reader = BytesIO(message_data)
        avro_reader = fastavro.reader(bytes_reader)

        # Extract records
        records = [record for record in avro_reader]
        if not records:
            logging.warning("No records found in the Avro message.")
            return None

        # Convert records to CSV format
        fieldnames = list(records[0].keys())
        csv_buffer = io.StringIO()
        writer = csv.DictWriter(csv_buffer, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(records)

        # Generate a unique filename
        unique_id = uuid.uuid4()
        filename = f"message_{unique_id}.csv"

        logging.info(f"Processed message into file: {filename}")
        return filename, csv_buffer.getvalue()
    except Exception as e:
        logging.error(f"Error processing Avro message: {e}")
        return None

def write_csv_to_gcs(element):
    global pipeline_options
    from apache_beam.io.gcp.gcsio import GcsIO
    try:
        filename, csv_data = element
        if not csv_data:
            logging.warning(f"No CSV data to write for {filename}. Skipping.")
            return

        # Write CSV data to GCS
        gcs_io = GcsIO()
        with gcs_io.open(pipeline_options.output_path.get(), 'w') as gcs_file:
            gcs_file.write(csv_data.encode("utf-8"))

        logging.info(f"Successfully wrote {filename} to GCS: {pipeline_options.output_path.get()}")
    except Exception as e:
        logging.error(f"Error writing CSV to GCS: {e}")

def run_pipeline(argv = None):
    global pipeline_options
    parser = argparse.ArgumentParser()
    known_args, pipeline_args = parser.parse_known_args(argv)
    pipeline_options = BQToGCSOptions(pipeline_args)
    with beam.Pipeline(options=pipeline_options) as pipeline:
        subscription_string= pipeline_options.input_subscription.get()
        messages = (pipeline
                     
                    | "ReadFromPubSub" >> beam.io.ReadFromPubSub(subscription=subscription_string)
                   )
        csv_files = (messages
                     | "ProcessAvroToCSV" >> beam.Map(process_avro_to_csv)
                     )
        _ = (csv_files
             | "WriteCSVToGCS" >> beam.Map(write_csv_to_gcs))

if __name__ == '__main__':
    run_pipeline()
