import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
import argparse
import logging

def run(argv=None):
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--query',
        required=True,
        help='BigQuery SQL query to execute'
    )
    parser.add_argument(
        '--project',
        required=True,
        help='GCP Project ID'
    )
    
    known_args, pipeline_args = parser.parse_known_args(argv)
    pipeline_options = PipelineOptions(pipeline_args)
    logging.getLogger().setLevel(logging.INFO)
    with beam.Pipeline(options=pipeline_options) as pipeline:
        (pipeline 
         | 'ReadFromBigQuery' >> beam.io.ReadFromBigQuery(
             query=known_args.query,
             use_standard_sql=True,
             project=known_args.project)
         | 'LogRows' >> beam.Map(lambda row: logging.info(f"Row Data: {row}"))
        )

if __name__ == '__main__':
    run()



command to run above code in local:
python3 pipeline.py \
--runner=DirectRunner \
--project vz-it-np-gudv-dev-vzntdo-0 \
--temp_location=gs://vznet-test/wireline_churn_test/tmp/ \
--template-file-gcs-location gs://vznet-test/wireline_churn_test/src/template.json \
--region us-east4 \
--query="SELECT * FROM vz-it-np-gudv-dev-vzntdo-0.vzn_nsdl_common_core_tbls.echo_ticket_opened_proc_v0 LIMIT 10;"

commands to build as riun flex template:
gcloud dataflow flex-template build \
gs://vznet-test/wireline_churn_test/src/template.json \
--image us-east4-docker.pkg.dev/vz-it-np-gudv-dev-vzntdo-0/vznet/pipeline_env:V1 \
--sdk-language "PYTHON" \
--metadata-file /home/udayka/workspace/metadata.json

gcloud dataflow flex-template run bigquery123 \
--template-file-gcs-location gs://vznet-test/wireline_churn_test/src/template.json \
--region us-east4 \
--parameters query="SELECT * FROM vz-it-np-gudv-dev-vzntdo-0.vzn_nsdl_common_core_tbls.echo_ticket_opened_proc_v0 LIMIT 10",project="vz-it-np-gudv-dev-vzntdo-0" \
--temp-location=gs://vznet-test/wireline_churn_test/tmp/ \
--num-workers 2 \
--max-workers 2 \
--worker-machine-type n2-standard-2 \
--disable-public-ips \
--network=shared-np-east \
--dataflow-kms-key=projects/vz-it-np-d0sv-vsadkms-0/locations/us-east4/keyRings/vz-it-np-kr-aid/cryptoKeys/vz-it-np-kms-gudv \
--subnetwork=https://www.googleapis.com/compute/v1/projects/vz-it-np-exhv-sharedvpc-228116/regions/us-east4/subnetworks/shared-np-east-green-subnet-2 \
--service-account-email sa-dev-gudv-app-vzntdo-0@vz-it-np-gudv-dev-vzntdo-0.iam.gserviceaccount.com


the metadata file: {
    "name": "BigQuery Read Pipeline",
    "description": "A pipeline that reads from BigQuery and logs the results",
    "parameters": [
        {
            "name": "query",
            "label": "BigQuery Query",
            "helpText": "The SQL query to execute in BigQuery",
            "isOptional": false,
            "regexes": [],
            "paramType": "TEXT"
        },
        {
            "name": "project",
            "label": "GCP Project ID",
            "helpText": "The Google Cloud project ID where BigQuery is located",
            "isOptional": false,
            "regexes": [],
            "paramType": "TEXT"
        }
    ]
}

and my docker image creation file:
# Use Apache Beam's official Python SDK image
FROM apache/beam_python3.9_sdk:2.48.0

# Set working directory
WORKDIR /workspace

# Install additional system dependencies if needed
RUN apt-get update && apt-get install -y \
    build-essential \
    libffi-dev \
    libssl-dev \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Create non-root user
RUN groupadd -r appuser && useradd -r -g appuser appuser

# Copy requirements file
COPY requirements.txt /workspace/requirements.txt

# Install Python dependencies
RUN pip install --upgrade pip && \
    pip install --no-cache-dir -r /workspace/requirements.txt

# Copy pipeline code
COPY dfpipeline.py /workspace/pipeline.py

# Set permissions
RUN chown -R appuser:appuser /workspace && \
    mkdir -p /home/appuser/.local && \
    chown -R appuser:appuser /home/appuser

# Switch to non-root user
USER appuser

# Set Python path to include user site-packages
ENV PYTHONPATH=/home/appuser/.local/lib/python3.9/site-packages:$PYTHONPATH

# Use the launcher that comes with the Apache Beam SDK image
ENTRYPOINT ["python", "/workspace/pipeline.py"]

error I am getting in dataflow:
docker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: "/opt/google/dataflow/python_template_launcher": stat /opt/google/dataflow/python_template_launcher: no such file or directory: unknown.
