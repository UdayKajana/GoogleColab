import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.io.gcp.bigquery import ReadFromBigQuery
import argparse
import logging

logging.getLogger().setLevel(logging.ERROR)

def run(argv=None):
    parser = argparse.ArgumentParser()
    parser.add_argument('--query', required=True, help='BigQuery SQL query')
    parser.add_argument('--gcs_folder_path', required=True, help='Output GCS folder path')
    known_args, pipeline_args = parser.parse_known_args(argv)

    pipeline_options = PipelineOptions(pipeline_args)
    
    with beam.Pipeline(options=pipeline_options) as p:
        # Reading from BigQuery
        data = p | 'ReadFromBigQuery' >> ReadFromBigQuery(
            query=known_args.query,
            use_standard_sql=True
        )
        
        # Converting to CSV format
        csv_data = data | 'ConvertToCSV' >> beam.Map(
            lambda row: ','.join(str(x) for x in row.values())
        )
        
        # Writing to GCS
        csv_data | 'WriteToGCS' >> beam.io.WriteToText(
            f'{known_args.gcs_folder_path}/output.csv',
            file_name_suffix='.csv',
            header="ont_activation_date,data_circuit_id,circuit_id,video_circuit_id,service_type,address_id,vision_account_id,vision_customer_id,address_type,line_of_business"
        )

if __name__ == '__main__':
    run()


gcloud dataflow flex-template build $TEMPLATE_PATH \
    --source $PIPELINE_FOLDER/pipeline.py \
    --runner DataflowRunner \
    --staging-location $PIPELINE_FOLDER/staging \
    --temp-location $PIPELINE_FOLDER/temp \
    --python-path $PIPELINE_FOLDER \
    --requirements-file requirements.txt \
    --template-spec-path $PIPELINE_FOLDER/template_spec.json



gcloud dataflow flex-template run "bigquery-to-gcs-job-$(date +%Y%m%d-%H%M%S)" \
    --template-file-gcs-location $TEMPLATE_PATH \
    --region $REGION \
    --parameters query="SELECT * FROM vz-it-np-gudv-dev-vzntdo-0.vzn_nsdl_common_core_tbls.echo_ticket_opened_proc_v0 LIMIT 10" \
    --parameters gcs_folder_path="gs://vznet-test/wireline_churn_test/output/" \
    --network shared-np-east \
    --subnetwork https://www.googleapis.com/compute/v1/projects/vz-it-np-exhv-sharedvpc-228116/regions/us-east4/subnetworks/shared-np-east-green-subnet-2 \
    --service-account-email sa-dev-gudv-app-vzntdo-0@vz-it-np-gudv-dev-vzntdo-0.iam.gserviceaccount.com \
    --num-workers 2 \
    --max-workers 2 \
    --worker-machine-type n2-standard-2


{
    "name": "BigQuery to GCS Pipeline",
    "description": "A pipeline that reads from BigQuery and writes to GCS",
    "parameters": [
        {
            "name": "query",
            "label": "BigQuery SQL query",
            "helpText": "SQL query to execute",
            "paramType": "TEXT",
            "isOptional": false
        },
        {
            "name": "gcs_folder_path",
            "label": "Output GCS folder path",
            "helpText": "GCS location to write output files",
            "paramType": "TEXT",
            "isOptional": false
        }
    ],
    "defaultEnvironment": {
        "tempLocation": "gs://vznet-test/wireline_churn_test/temp",
        "stagingLocation": "gs://vznet-test/wireline_churn_test/staging",
        "zone": "us-east4-a",
        "numWorkers": 2,
        "maxWorkers": 2,
        "network": "shared-np-east",
        "subnetwork": "https://www.googleapis.com/compute/v1/projects/vz-it-np-exhv-sharedvpc-228116/regions/us-east4/subnetworks/shared-np-east-green-subnet-2",
        "machineType": "n2-standard-2",
        "serviceAccountEmail": "sa-dev-gudv-app-vzntdo-0@vz-it-np-gudv-dev-vzntdo-0.iam.gserviceaccount.com",
        "additionalExperiments": ["use_runner_v2"],
        "additionalUserLabels": {
            "purpose": "bigquery-export",
            "team": "data-engineering"
        }
    },
    "sdkInfo": {
        "language": "PYTHON"
    }
}
