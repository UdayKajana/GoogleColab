FROM apache/beam_python3.9_sdk:2.61.0

# Install Java for running the expansion service
RUN apt-get update && \
    apt-get install -y openjdk-11-jdk && \
    apt-get clean

# Download the SQL expansion service JAR
RUN wget -P /opt/apache/beam/jars https://repo1.maven.org/maven2/org/apache/beam/beam-sdks-java-extensions-sql-expansion-service/2.61.0/beam-sdks-java-extensions-sql-expansion-service-2.61.0.jar

# Copy the launcher code
COPY pipeline_launcher.py /template/pipeline_launcher.py

# Set the required metadata labels
LABEL "template.name"="beam-sql-flex-template"
LABEL "template.version"="1.0.0"
LABEL "template.description"="Apache Beam SQL Flex Template with expansion service"

# Set the entrypoint
ENTRYPOINT ["python", "/template/pipeline_launcher.py"]


import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
import subprocess
import time
import logging
import argparse
from typing import Any, Dict

class SQLFlexTemplateOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
        parser.add_argument('--gcs_pipeline_path',
                          required=True,
                          help='GCS path to the pipeline file')
        parser.add_argument('--expansion_service_port',
                          default='8097',
                          help='Port for the SQL expansion service')

def start_expansion_service():
    """Start the SQL expansion service"""
    cmd = [
        'java', '-jar',
        '/opt/apache/beam/jars/beam-sdks-java-extensions-sql-expansion-service-2.61.0.jar',
        '--expansion_port', '8097'
    ]
    process = subprocess.Popen(cmd)
    time.sleep(5)  # Wait for service to start
    return process

def run_pipeline(pipeline_path: str, options: Dict[str, Any]):
    """Download and run the pipeline"""
    # Download pipeline file from GCS
    subprocess.run(['gsutil', 'cp', pipeline_path, '/tmp/current_pipeline.py'])
    
    # Import and run the pipeline
    import importlib.util
    spec = importlib.util.spec_from_file_location("pipeline", "/tmp/current_pipeline.py")
    pipeline_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(pipeline_module)
    
    # Run the pipeline's main function
    if hasattr(pipeline_module, 'run'):
        pipeline_module.run(options)
    else:
        raise ValueError("Pipeline file must contain a 'run' function")

def run():
    logging.getLogger().setLevel(logging.INFO)
    
    # Parse the command-line arguments
    parser = argparse.ArgumentParser()
    known_args, pipeline_args = parser.parse_known_args()
    
    # Create pipeline options
    options = SQLFlexTemplateOptions(pipeline_args)
    
    # Start expansion service
    expansion_service = start_expansion_service()
    
    try:
        # Run the pipeline
        run_pipeline(
            options.gcs_pipeline_path,
            {
                'expansion_service_port': options.expansion_service_port,
                **options.get_all_options()
            }
        )
    finally:
        # Clean up
        expansion_service.terminate()

if __name__ == '__main__':
    run()



{
  "image": "gcr.io/YOUR_PROJECT/beam-sql-flex",
  "metadata": {
    "name": "Beam SQL Flex Template",
    "description": "Flex template for running Beam SQL pipelines with expansion service",
    "parameters": [
      {
        "name": "gcs_pipeline_path",
        "label": "Pipeline File Path",
        "helpText": "GCS path to the pipeline file to execute",
        "regexes": ["^gs://.*\\.py$"],
        "isOptional": false
      }
    ]
  },
  "sdkInfo": {
    "language": "PYTHON"
  }
}



gcloud dataflow flex-template build \
    gs://YOUR_BUCKET/templates/beam-sql-flex.json \
    --image gcr.io/YOUR_PROJECT/beam-sql-flex \
    --sdk-language PYTHON \
    --metadata-file template.json



gcloud dataflow flex-template run "beam-sql-job-$(date +%Y%m%d-%H%M%S)" \
    --template-file-gcs-location gs://YOUR_BUCKET/templates/beam-sql-flex.json \
    --parameters gcs_pipeline_path=gs://YOUR_BUCKET/pipelines/your_pipeline.py \
    --region=YOUR_REGION





import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
import argparse
import logging
from typing import Dict, Any

def run(custom_options: Dict[str, Any] = None):
    """Main pipeline function that writes sample data and queries it"""
    
    # Create pipeline options
    parser = argparse.ArgumentParser()
    known_args, pipeline_args = parser.parse_known_args()
    
    pipeline_options = PipelineOptions(pipeline_args)
    if custom_options:
        pipeline_options.display_data = custom_options

    # Sample data - you can modify this as needed
    sample_data = [
        {'id': 1, 'name': 'John', 'department': 'IT', 'salary': 75000},
        {'id': 2, 'name': 'Alice', 'department': 'HR', 'salary': 65000},
        {'id': 3, 'name': 'Bob', 'department': 'IT', 'salary': 80000},
        {'id': 4, 'name': 'Carol', 'department': 'Finance', 'salary': 90000},
    ]

    # Schema for the data
    schema = 'id:INTEGER, name:STRING, department:STRING, salary:INTEGER'

    with beam.Pipeline(options=pipeline_options) as p:
        # Write initial data to BigQuery
        initial_data = (p 
            | 'CreateInitialData' >> beam.Create(sample_data)
            | 'WriteToBigQuery' >> beam.io.WriteToBigQuery(
                'your-project:your_dataset.employees',  # Replace with your table
                schema=schema,
                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND
            ))

        # Convert data for SQL processing
        employees = (p
            | 'CreateData' >> beam.Create(sample_data)
            | 'ToRows' >> beam.ToRow())

        # Register the PCollection as a table
        employees.schema = schema

        # Example SQL query using the expansion service
        sql_query = """
            SELECT 
                department,
                COUNT(*) as employee_count,
                AVG(salary) as avg_salary
            FROM PCOLLECTION
            GROUP BY department
            HAVING COUNT(*) > 0
        """

        # Apply SQL transform using expansion service
        query_results = employees | 'SQLQuery' >> beam.ExternalTransform(
            'beam:transform:org.apache.beam.sdk.extensions.sql:sql',
            {'query': sql_query},
            f'localhost:{pipeline_options.expansion_service_port}'
        )

        # Write query results to a different BigQuery table
        query_results | 'WriteResultsToBigQuery' >> beam.io.WriteToBigQuery(
            'your-project:your_dataset.department_summary',  # Replace with your table
            schema='department:STRING, employee_count:INTEGER, avg_salary:FLOAT',
            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND
        )

if __name__ == '__main__':
    logging.getLogger().setLevel(logging.INFO)
    run()
