# Use official Python image as base
FROM apache/beam_python3.8_sdk:2.61.0

# Set working directory
WORKDIR /pipeline

# Copy the expansion service JAR
ADD https://repo1.maven.org/maven2/org/apache/beam/beam-sdks-java-extensions-sql-expansion-service/2.61.0/beam-sdks-java-extensions-sql-expansion-service-2.61.0.jar /opt/apache/beam/jars/

# Copy pipeline code and requirements
COPY pipeline/ ./

# Install dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Set environment variables
ENV EXPANSION_SERVICE_PORT=8097
ENV BEAM_SQL_EXPANSION_JAR=/opt/apache/beam/jars/beam-sdks-java-extensions-sql-expansion-service-2.61.0.jar

# Start the expansion service
RUN nohup java -jar $BEAM_SQL_EXPANSION_JAR --expansion-service-port=$EXPANSION_SERVICE_PORT &

# Set the entrypoint
ENTRYPOINT ["python", "main.py"]


apache-beam[gcp]==2.61.0
apache-beam-client==2.61.0



import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
import logging
import json

class CustomPipelineOptions(PipelineOptions):
    @classmethod
    def _add_argparse_args(cls, parser):
        parser.add_value_provider_argument(
            '--temp_location',
            help='GCS Temp directory for the pipeline'
        )
        parser.add_value_provider_argument(
            '--sql_expansion_service',
            default='localhost:8097',
            help='SQL Expansion Service address'
        )

def run_pipeline(pipeline_options):
    # Sample data
    sample_data = [
        {'id': 1, 'name': 'John', 'age': 30},
        {'id': 2, 'name': 'Jane', 'age': 25},
        {'id': 3, 'name': 'Bob', 'age': 35}
    ]

    with beam.Pipeline(options=pipeline_options) as pipeline:
        # Create initial PCollection
        records = pipeline | "CreateData" >> beam.Create(sample_data)
        
        # Convert to Row format for SQL
        rows = records | "ToRows" >> beam.Map(
            lambda x: beam.Row(
                id=x['id'],
                name=x['name'],
                age=x['age']
            )
        )

        # Apply SQL transform
        sql_results = rows | "SQLTransform" >> beam.transforms.SqlTransform(
            """
            SELECT id, name, age 
            FROM PCOLLECTION 
            WHERE age > 25
            """,
            expansion_service=pipeline_options.sql_expansion_service
        )

        # Convert results back to dictionary
        final_results = sql_results | "ToDict" >> beam.Map(
            lambda row: {
                'id': row.id,
                'name': row.name,
                'age': row.age
            }
        )

        # Write results (for example, to console)
        final_results | "Print" >> beam.Map(print)

if __name__ == '__main__':
    logging.getLogger().setLevel(logging.INFO)
    pipeline_options = CustomPipelineOptions()
    run_pipeline(pipeline_options)



{
    "name": "Beam SQL Pipeline",
    "description": "Apache Beam pipeline using SQL expansion service",
    "parameters": [
        {
            "name": "temp_location",
            "label": "Temporary Location",
            "helpText": "GCS temporary location for the pipeline",
            "regexes": ["^gs://[a-z0-9-_.]+/[a-z0-9-_./]+$"],
            "required": true
        },
        {
            "name": "sql_expansion_service",
            "label": "SQL Expansion Service",
            "helpText": "Address of the SQL expansion service",
            "default": "localhost:8097"
        }
    ],
    "sdkInfo": {
        "language": "PYTHON"
    }
}





# First, build and push the Docker image
docker build -t gcr.io/[PROJECT]/beam-sql-pipeline .
docker push gcr.io/[PROJECT]/beam-sql-pipeline

# Create the Flex Template
gcloud dataflow flex-template build \
  gs://[BUCKET]/templates/beam-sql-pipeline.json \
  --image gcr.io/[PROJECT]/beam-sql-pipeline \
  --sdk-language PYTHON \
  --metadata-file metadata.json

# Launch the template
gcloud dataflow flex-template run "beam-sql-job-$(date +%Y%m%d-%H%M%S)" \
  --template-file-gcs-location gs://[BUCKET]/templates/beam-sql-pipeline.json \
  --region [REGION] \
  --parameters temp_location=gs://[BUCKET]/temp
