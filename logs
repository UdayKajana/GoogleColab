from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def my_python_function():
    # Your Python code logic here
    print("Executing local Python code")
    # Add your data processing logic
    return "Success"

dag = DAG(
    'local_python_execution',
    default_args=default_args,
    description='DAG to run Python code locally',
    schedule_interval=timedelta(days=1)
)

run_python = PythonOperator(
    task_id='execute_python_code',
    python_callable=my_python_function,
    dag=dag
)





from airflow import DAG
from airflow.providers.google.cloud.operators.dataflow import DataflowCreatePythonJobOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'dataflow_python_job',
    default_args=default_args,
    description='DAG to trigger Dataflow Python job',
    schedule_interval=timedelta(days=1)
)

dataflow_task = DataflowCreatePythonJobOperator(
    task_id='dataflow_python_execution',
    py_file='gs://your-bucket/path/to/dataflow_script.py',
    job_name='dataflow-python-job-{{ds_nodash}}',
    location='us-central1',
    project_id='your-project-id',
    gcp_conn_id='google_cloud_default',
    options={
        'temp_location': 'gs://your-bucket/temp',
        'staging_location': 'gs://your-bucket/staging',
        'region': 'us-central1'
    },
    dag=dag
)
